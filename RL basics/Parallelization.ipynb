{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP4M4DsPQLiRufU2OsrJon0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Synchronous parallel environment\n","\n","In our previous implementation, we run one episode at a time. To accelerate learning, we now leverage synchronous parallel environment execution, utilizing GPU parallelism to run multiple episodes simultaneously. This approach allows us to gather diverse experiences more rapidly within the same time frame.\n","\n","The psuedocode of the below code is as follow:\n","1. Execute `num_envs` (default: 16) environments in parallel using JAX's vectorization (vmap) on the GPU.\n","\n","2. Collect and store the experiences (states, actions, rewards, next states) from all parallel environments at each step.\n","\n","3. Once the total number of collected experiences reaches `batch_size`, compute the Temporal Difference (TD) loss by averaging the error across the entire batch.\n","\n","4. Perform a gradient-based update using this averaged loss and repeat the process.\n","\n","This method provides two key advantages:\n","- Faster experience collection: By running multiple environments in parallel, we generate a larger and more diverse set of experiences without increasing wall-clock time.\n","\n","- More stable and accurate Updates: Calculating the TD error over a batch of experiences, rather than a single instance, provides a better estimate of the true gradient, leading to more robust and effective policy updates."],"metadata":{"id":"BE40TdLm_t12"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-emY92DplazC","outputId":"707be4f4-956e-41f3-e247-19aa9b803017"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0, No episodes completed yet\n","Step 100, Avg reward: 20.7, Total episodes: 57\n","Step 200, Avg reward: 27.3, Total episodes: 99\n","Step 300, Avg reward: 16.9, Total episodes: 219\n","Step 400, Avg reward: 21.8, Total episodes: 257\n","Step 500, Avg reward: 36.8, Total episodes: 306\n","Step 600, Avg reward: 11.3, Total episodes: 448\n","Step 700, Avg reward: 16.5, Total episodes: 509\n","Step 800, Avg reward: 23.8, Total episodes: 519\n","Step 900, Avg reward: 41.4, Total episodes: 532\n","Step 1000, Avg reward: 58.5, Total episodes: 565\n","Step 1100, Avg reward: 67.5, Total episodes: 582\n","Step 1200, Avg reward: 74.5, Total episodes: 589\n","Step 1300, Avg reward: 96.4, Total episodes: 606\n","Step 1400, Avg reward: 99.4, Total episodes: 617\n","Step 1500, Avg reward: 95.4, Total episodes: 628\n","Step 1600, Avg reward: 98.7, Total episodes: 653\n","Step 1700, Avg reward: 104.4, Total episodes: 662\n","Step 1800, Avg reward: 109.5, Total episodes: 664\n","Step 1900, Avg reward: 123.6, Total episodes: 676\n","Step 2000, Avg reward: 134.1, Total episodes: 691\n","Step 2100, Avg reward: 132.7, Total episodes: 695\n","Step 2200, Avg reward: 138.4, Total episodes: 707\n","Step 2300, Avg reward: 142.6, Total episodes: 713\n","Step 2400, Avg reward: 150.0, Total episodes: 725\n","Step 2500, Avg reward: 153.5, Total episodes: 731\n","Step 2600, Avg reward: 162.7, Total episodes: 741\n","Step 2700, Avg reward: 167.7, Total episodes: 744\n"]}],"source":["import jax\n","import jax.numpy as jnp\n","import optax\n","import gymnax\n","from gymnax.wrappers.purerl import FlattenObservationWrapper\n","import jax.tree_util as jtu\n","\n","# Wrap the environment to ensure consistent observation shapes\n","env, env_params = gymnax.make(\"CartPole-v1\")\n","env = FlattenObservationWrapper(env)\n","env_params = env_params\n","\n","def rbf_features(x, centers, sigma=0.5):\n","    # x: (d,) or (batch_size, d), centers: (n_centers, d)\n","\n","    # Normalize input to appropriate range for CartPole\n","    x= x / jnp.array([2.4, 3.0, 0.2, 3.0])  # CartPole observation scaling\n","\n","    diffs = x[None] - centers if x.ndim == 1 else x[:, None] - centers\n","    sq_dist = jnp.sum(diffs**2, axis=-1)\n","    return jnp.exp(-sq_dist / (2 * sigma**2))\n","\n","def init_params(rng, n_features, n_actions):\n","    W = jax.random.normal(rng, (n_features, n_actions)) * 0.1\n","    return W\n","\n","def q_values(W, obs, centers, sigma=0.5):\n","    phi = rbf_features(obs, centers, sigma)  # (batch_size, n_features) or (n_features,)\n","    return jnp.dot(phi, W)  # (batch_size, n_actions) or (n_actions,)\n","\n","def select_action(W, obs, rng, centers, sigma=0.5, epsilon=0.1):\n","    q = q_values(W, obs, centers, sigma)\n","    greedy = jnp.argmax(q, axis=-1)\n","    explore = jax.random.bernoulli(rng, epsilon, shape=greedy.shape)\n","    random_actions = jax.random.randint(rng, greedy.shape, 0, q.shape[-1])\n","    return jnp.where(explore, random_actions, greedy)\n","\n","def td_loss(W, obs, action, reward, next_obs, done, gamma, centers, sigma):\n","    q = q_values(W, obs, centers, sigma)\n","    q_selected = jnp.take_along_axis(q, action[:, None], axis=-1).squeeze()\n","\n","    next_q = jnp.max(q_values(W, next_obs, centers, sigma), axis=-1)\n","    target = reward + gamma * (1 - done) * next_q\n","\n","    return jnp.mean(0.5 * (q_selected - target) ** 2)\n","\n","def train_parallel_simple(num_episodes=500, lr=1e-2, gamma=0.99, n_centers=50,\n","                         sigma=0.5, num_envs=16, batch_size=16):\n","    obs_dim = env.observation_space(env_params).shape[0]\n","    n_actions = env.action_space(env_params).n\n","\n","    rng = jax.random.PRNGKey(0)\n","    rng, centers_rng, init_rng = jax.random.split(rng, 3)\n","\n","    # Random RBF centers\n","    centers = jax.random.uniform(centers_rng, (n_centers, obs_dim), minval=-1, maxval=1)\n","    W = init_params(init_rng, n_centers, n_actions)\n","\n","    opt = optax.adam(lr)\n","    opt_state = opt.init(W)\n","\n","    # Vectorized functions\n","    vmap_reset = jax.vmap(env.reset, in_axes=(0, None))\n","    vmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n","    vmap_select_action = jax.vmap(select_action, in_axes=(None, 0, 0, None, None, None))\n","\n","    @jax.jit\n","    def update_batch(W, opt_state, batch_obs, batch_actions, batch_rewards,\n","                    batch_next_obs, batch_dones):\n","        def batch_loss(W):\n","            obs = batch_obs.reshape(-1, obs_dim)\n","            actions = batch_actions.reshape(-1)\n","            rewards = batch_rewards.reshape(-1)\n","            next_obs = batch_next_obs.reshape(-1, obs_dim)\n","            dones = batch_dones.reshape(-1)\n","            return td_loss(W, obs, actions, rewards, next_obs, dones, gamma, centers, sigma)\n","\n","        grads = jax.grad(batch_loss)(W)\n","        updates, opt_state = opt.update(grads, opt_state, W)\n","        W = optax.apply_updates(W, updates)\n","        return W, opt_state\n","\n","    # Initialize environments\n","    rng, *env_rngs = jax.random.split(rng, num_envs + 1)\n","    obs, states = vmap_reset(jnp.array(env_rngs), env_params)\n","\n","    episode_rewards = jnp.zeros(num_envs)\n","    all_rewards = []\n","\n","    # Initialize batch storage\n","    batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = [], [], [], [], []\n","\n","    for step in range(100000):  # Large enough to collect required episodes\n","        rng, action_rng = jax.random.split(rng)\n","        action_rngs = jax.random.split(action_rng, num_envs)\n","\n","        # Select actions for all environments\n","        actions = vmap_select_action(W, obs, jnp.array(action_rngs), centers, sigma, 0.1)\n","\n","        # Step all environments\n","        rng, *step_rngs = jax.random.split(rng, num_envs + 1)\n","        next_obs, next_states, rewards, dones, _ = vmap_step(\n","            jnp.array(step_rngs), states, actions, env_params\n","        )\n","\n","        # Store experience for batch update\n","        batch_obs.append(obs)\n","        batch_actions.append(actions)\n","        batch_rewards.append(rewards)\n","        batch_next_obs.append(next_obs)\n","        batch_dones.append(dones.astype(jnp.float32))  # Convert to float for consistency\n","\n","        # Update episode rewards\n","        episode_rewards += rewards\n","\n","        # Record completed episodes and reset counters for done envs\n","        completed_mask = dones.astype(bool)\n","        completed_rewards = episode_rewards[completed_mask]\n","        all_rewards.extend(completed_rewards.tolist())\n","\n","        episode_rewards = episode_rewards * (1 - dones.astype(jnp.float32))\n","\n","        # Compute resets for all (unconditionally for simplicity)\n","        rng, reset_rng = jax.random.split(rng)\n","        reset_rngs = jax.random.split(reset_rng, num_envs)\n","        reset_obs, reset_states = vmap_reset(jnp.array(reset_rngs), env_params)\n","\n","        # Apply resets only to done environments\n","        obs = jnp.where(dones[:, None], reset_obs, next_obs)\n","        states = jtu.tree_map(lambda r, n: jnp.where(dones, r, n), reset_states, next_states)\n","\n","        # Perform batch update when we have enough experience\n","        if len(batch_obs) >= batch_size:\n","            batch_obs_arr = jnp.stack(batch_obs)\n","            batch_actions_arr = jnp.stack(batch_actions)\n","            batch_rewards_arr = jnp.stack(batch_rewards)\n","            batch_next_obs_arr = jnp.stack(batch_next_obs)\n","            batch_dones_arr = jnp.stack(batch_dones)\n","\n","            W, opt_state = update_batch(\n","                W, opt_state, batch_obs_arr, batch_actions_arr,\n","                batch_rewards_arr, batch_next_obs_arr, batch_dones_arr\n","            )\n","\n","            # Clear batch\n","            batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = [], [], [], [], []\n","\n","        # Print progress\n","        if step % 100 == 0:\n","            if all_rewards:\n","                recent_rewards = all_rewards[-100:] if len(all_rewards) > 100 else all_rewards\n","                avg_reward = jnp.mean(jnp.array(recent_rewards))\n","                print(f\"Step {step}, Avg reward: {avg_reward:.1f}, \"\n","                      f\"Total episodes: {len(all_rewards)}\")\n","\n","                # Early stopping if solved\n","                if len(all_rewards) >= 100 and avg_reward >= 475.0:\n","                    print(\"CartPole solved!\")\n","                    break\n","            else:\n","                print(f\"Step {step}, No episodes completed yet\")\n","\n","        # Stop if we've collected enough episodes\n","        if len(all_rewards) >= num_episodes:\n","            break\n","\n","    return W, centers, all_rewards\n","\n","# Run training\n","W, centers, rewards = train_parallel_simple(num_episodes=50000, num_envs=16, n_centers=500, lr=5e-3, batch_size=16)\n","print(f\"Final average reward: {jnp.mean(jnp.array(rewards[-100:])):.1f}\")"]}]}