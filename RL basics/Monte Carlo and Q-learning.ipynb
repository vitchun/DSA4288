{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOv70GW9EL5+7x+0sGoWnrP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"d0AufvPgS7kX","executionInfo":{"status":"ok","timestamp":1754819125748,"user_tz":-480,"elapsed":5165,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"outputs":[],"source":["import gymnasium as gym\n","import jax\n","import jax.numpy as jnp\n","import numpy as np"]},{"cell_type":"markdown","source":["# Monte Carlo"],"metadata":{"id":"nG0tyEcL5Vg9"}},{"cell_type":"markdown","source":["The Monte Carlo method estimates the value function for a given policy by estimating it from \"experience\", that is to simply averaging the returns observed after (first) visit to that state.\n","\n","Suppose we wish to estimate $v_\\pi(s)$, the first-visit MC method estimates $v_\\pi(s)$ as the average of the returns following first visits to $s$ (note that $s$ can be visited multiple times in the same episode).\n","\n","Here is the pseudocode:\n","1. Initialize:\n","- $\\pi$ $\\leftarrow$ policy to be evaluated\n","- $V$ $\\leftarrow$ an arbitrary state-value function\n","- $Returns(s)$ $\\leftarrow$ an ampty list, for all $s \\in S$\n","\n","2. Repeat forever:\n","- Generate an episode using $\\pi$\n","- For each state $s$ appearing in the epsiode:\n","  - $G$ $\\leftarrow$ return following the first occurence of $s$\n","  - Append $G$ to $Returns(s)$\n","  - $V(s)$ $\\leftarrow$ average($Returns(s)$)\n","\n","For Generalized Policy Iteration (GPI), we simply update the policy functuon $\\pi$ after each episode by performing $\\pi \\leftarrow argmax_aQ(s,a)$ where $Q(s,a)$ is the action-value function."],"metadata":{"id":"H_i_2KPjYVy8"}},{"cell_type":"markdown","source":["However, note that to store all the returns and compute the average can be memory heavy and slow, hence implementing the MC method, we often use the following:\n","\n","$$Q(s,a)_{new} ← Q(s,a)_{old} + \\alpha (G_t - Q(s,a)_{old}) $$\n","\n","Note that if we set $\\alpha$ to be $\\frac{1}{N(s,a)}$ where $N(s,a)$ is the number of visits for the state $s$ and action $a$, then we obtain the running average update, which is more memory efficient.\n","\n","However, in practice, the $\\alpha$ can be chosen to be other number. In fact, it is a hyperparameter that can be tweaked to enhance the learning. If $\\alpha$ is small, the learning process is slower but more stable, as it moves towards the target slower, and vice versa if it is big."],"metadata":{"id":"-d_a6wzg0NrL"}},{"cell_type":"markdown","source":["Lastly, to avoid the algorithm from always choosing the same action (due to argmax), we can use the epsilon-greedy algoritm. This ensures that we have a balance of exploration and exploitation. We can do this by ensuring that for a set $\\epsilon$ probability, we make sure to randomly choose the action $a$ from the action space given state $s$."],"metadata":{"id":"qM-WdF3baOqk"}},{"cell_type":"code","source":["gamma = 1.0 #no discount factor\n","alpha = 0.1 #learning rate\n","epsilon = 0.1 #epsilon greedy algo\n","num_episodes = 500000"],"metadata":{"id":"kOcjsbjVXwiK","executionInfo":{"status":"ok","timestamp":1754819137735,"user_tz":-480,"elapsed":46,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"Blackjack-v1\", sab=True)"],"metadata":{"id":"4eZfsozPX0As","executionInfo":{"status":"ok","timestamp":1754819150833,"user_tz":-480,"elapsed":33,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["Q = {}\n","def get_Q(state, action):\n","    return Q.get((state, action), 0.0)"],"metadata":{"id":"UrwwRR9HX1-3","executionInfo":{"status":"ok","timestamp":1754819158866,"user_tz":-480,"elapsed":33,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def policy(state):\n","    if np.random.rand() < epsilon:\n","        return env.action_space.sample()\n","    q_vals = [get_Q(state, a) for a in range(env.action_space.n)]\n","    return int(np.argmax(q_vals))"],"metadata":{"id":"0cwt_q73X5Y4","executionInfo":{"status":"ok","timestamp":1754819172860,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["for ep in range(num_episodes):\n","    episode = []\n","    state, _ = env.reset()\n","    done = False\n","\n","    # Generate full episode\n","    while not done:\n","        action = policy(state)\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        episode.append((state, action, reward))\n","        state = next_state\n","        done = terminated or truncated\n","\n","    # First-visit MC updates\n","    G = 0\n","    visited = set()\n","    for t in reversed(range(len(episode))):\n","        s, a, r = episode[t]\n","        G = gamma * G + r\n","        if (s, a) not in visited:\n","            visited.add((s, a))\n","            old_q = get_Q(s, a)\n","            Q[(s, a)] = old_q + alpha * (G - old_q)\n","\n","print(\"Learned Q for state (20, 10, False):\")\n","for a in range(env.action_space.n):\n","    print(a, get_Q((20, 10, False), a))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_xXAdSxX7OG","executionInfo":{"status":"ok","timestamp":1754819248848,"user_tz":-480,"elapsed":68665,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"fad97de9-6bff-47e5-94ea-0e852bf0a47e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Learned Q for state (20, 10, False):\n","0 0.6873336141764251\n","1 -0.5051139654361626\n"]}]},{"cell_type":"markdown","source":["# Temporal Difference Learning"],"metadata":{"id":"YGvBCh6r5Yjw"}},{"cell_type":"markdown","source":["Instead of running the full episode in Monte Carlo methodm, temporal difference (TD) learning wait only until the next time step, that is, after $t+1$ they immediately form a target and make an useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. This is the simplest TD method, known as $TD(0)$, which can be adjusted to include more time steps up to $t+n$, called $n$ step TD learning. The update is as follows\n","\n","$$\n","Q(s,a) \\leftarrow Q(s,a) + \\alpha (R_{t+1} + \\gamma Q(s', a') - Q(s,a)) $$\n","\n","The above is the SARSA method, which is an on-policy method. It learns the value of the policy it is currently following, including the exploration steps. The update rule for SARSA uses the Q-value of the next action actually taken by the agent's current policy (which includes exploration, e.g., using an epsilon-greedy strategy).\n","\n","Whereas for Q-leaning method, it is an off-policy method. It learns the value of the optimal policy regardless of the agent's current exploration strategy. The update rule for Q-learning uses the Q-value of the greedy action (the action with the maximum Q-value) in the next state, effectively learning the optimal policy even while exploring sub-optimally. The update rule is as follows:\n","\n","$$\n","Q(s,a) \\leftarrow Q(s,a) + \\alpha (R_{t+1} + \\gamma \\max_a Q(s', a') - Q(s,a)) $$\n"],"metadata":{"id":"NhPht5DV5zvN"}},{"cell_type":"code","source":["import gymnasium as gym\n","import random\n","import numpy as np\n","from collections import defaultdict\n","from tqdm import trange\n","\n","def make_epsilon_greedy_policy(Q, nA, epsilon):\n","    \"\"\"Returns a function that given a state returns an action using epsilon-greedy.\"\"\"\n","    def policy_fn(state):\n","        if random.random() < epsilon: # epsilon-greedy\n","            return random.randrange(nA)\n","        else:\n","            # choose greedy action (tie-break randomly)\n","            qvals = [Q[(state, a)] for a in range(nA)]\n","            max_q = max(qvals)\n","            max_actions = [a for a, q in enumerate(qvals) if q == max_q]\n","            return random.choice(max_actions)\n","    return policy_fn\n","\n","def run_agent(env_name=\"Blackjack-v1\",\n","              method=\"sarsa\",           # \"sarsa\" or \"q_learning\"\n","              num_episodes=200000,\n","              alpha=0.1,\n","              gamma=1.0,\n","              epsilon=0.1,\n","              eval_every=None):\n","    \"\"\"\n","    Train a tabular SARSA or Q-learning agent on Blackjack and return learned Q and a history of evaluation rewards.\n","    \"\"\"\n","    env = gym.make(env_name, sab=False)  # sab=False for standard Blackjack rules\n","    nA = env.action_space.n # number of possible actions (Action space)\n","\n","    # Q-value store with default 0.0\n","    Q = defaultdict(float) # initialize\n","\n","    eval_history = []\n","\n","    for ep in trange(num_episodes, desc=f\"Training {method}\"):\n","        state, _ = env.reset()\n","        # initial action for SARSA (on-policy) must be sampled from behavior policy\n","        policy = make_epsilon_greedy_policy(Q, nA, epsilon)\n","        action = policy(state)\n","\n","        done = False\n","        while not done:\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","\n","            if method == \"sarsa\":\n","                # choose next action via epsilon-greedy from next_state\n","                next_action = policy(next_state) if not done else None # epsilon greedy\n","                target = reward + (gamma * Q[(next_state, next_action)] if not done else 0.0)\n","                # update\n","                old_q = Q[(state, action)]\n","                Q[(state, action)] = old_q + alpha * (target - old_q)\n","\n","                # move to next step\n","                state = next_state\n","                action = next_action\n","\n","            elif method == \"q_learning\":\n","                # compute target using max_a' Q(s', a')\n","                if done:\n","                    target = reward\n","                else:\n","                    qvals_next = [Q[(next_state, a)] for a in range(nA)]\n","                    target = reward + gamma * max(qvals_next)\n","\n","                old_q = Q[(state, action)]\n","                Q[(state, action)] = old_q + alpha * (target - old_q)\n","\n","                # choose next action for behavior (ε-greedy) to continue exploring\n","                state = next_state\n","                action = policy(state) if not done else None\n","\n","            else:\n","                raise ValueError(\"method must be 'sarsa' or 'q_learning'\")\n","\n","            # optional: N[(state, action)] += 1  (if tracking visits)\n","\n","        # optional evaluation during training\n","        if eval_every is not None and (ep + 1) % eval_every == 0:\n","            avg_reward = evaluate_policy(env, Q, nA, num_episodes=5000)\n","            eval_history.append((ep + 1, avg_reward))\n","\n","    env.close()\n","    return Q, eval_history\n","\n","def evaluate_policy(env, Q, nA, num_episodes=5000):\n","    \"\"\"Evaluate the greedy policy derived from Q by running episodes and returning mean reward.\"\"\"\n","    total_reward = 0.0\n","    for _ in range(num_episodes):\n","        state, _ = env.reset()\n","        done = False\n","        while not done:\n","            # greedy action (break ties randomly)\n","            qvals = [Q[(state, a)] for a in range(nA)]\n","            max_q = max(qvals)\n","            max_actions = [a for a, q in enumerate(qvals) if q == max_q]\n","            action = random.choice(max_actions)\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","            total_reward += reward\n","            state = next_state\n","    return total_reward / num_episodes\n","\n","if __name__ == \"__main__\":\n","    # hyperparameters\n","    NUM_EPISODES = 200000\n","    ALPHA = 0.1\n","    GAMMA = 1.0\n","    EPSILON = 0.1\n","\n","    # Train SARSA\n","    Q_sarsa, eval_hist_sarsa = run_agent(method=\"sarsa\",\n","                                         num_episodes=NUM_EPISODES,\n","                                         alpha=ALPHA,\n","                                         gamma=GAMMA,\n","                                         epsilon=EPSILON,\n","                                         eval_every=50000)\n","\n","    # Train Q-learning\n","    Q_qlearn, eval_hist_q = run_agent(method=\"q_learning\",\n","                                      num_episodes=NUM_EPISODES,\n","                                      alpha=ALPHA,\n","                                      gamma=GAMMA,\n","                                      epsilon=EPSILON,\n","                                      eval_every=50000)\n","\n","    # Final evaluation\n","    env = gym.make(\"Blackjack-v1\", sab=False)\n","    nA = env.action_space.n\n","    avg_reward_sarsa = evaluate_policy(env, Q_sarsa, nA, num_episodes=20000)\n","    avg_reward_qlearn = evaluate_policy(env, Q_qlearn, nA, num_episodes=20000)\n","    env.close()\n","\n","    print(f\"\\nFinal greedy-policy average reward (SARSA):   {avg_reward_sarsa:.4f}\")\n","    print(f\"Final greedy-policy average reward (Q-learning): {avg_reward_qlearn:.4f}\")\n","\n","    # Example: show Q for one state\n","    example_state = (20, 10, False)  # player's sum=20, dealer showing 10, usable ace False\n","    print(\"\\nQ-values for state\", example_state)\n","    print(\" SARSA:\", [Q_sarsa[(example_state, a)] for a in range(nA)])\n","    print(\" Q-Learn:\", [Q_qlearn[(example_state, a)] for a in range(nA)])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPpRvcHv9G6v","executionInfo":{"status":"ok","timestamp":1754846286170,"user_tz":-480,"elapsed":66756,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"593ebe64-dc5d-4bd1-b8f4-4102c3a62a4b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["Training sarsa: 100%|██████████| 200000/200000 [00:33<00:00, 5937.57it/s]\n","Training q_learning: 100%|██████████| 200000/200000 [00:27<00:00, 7164.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Final greedy-policy average reward (SARSA):   -0.0862\n","Final greedy-policy average reward (Q-learning): -0.0682\n","\n","Q-values for state (20, 10, False)\n"," SARSA: [0.5618423406970627, -0.82055376955104]\n"," Q-Learn: [0.2508874480609621, -0.9807600251803228]\n"]}]}]}