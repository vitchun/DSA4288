{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOTV3dZbmMZF3XmXfwSmCs9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### Replay Buffer\n","\n","In our previous implementation of parallelization, during the update step, we made use of experiences from only the most recent episodes. The above approach may lead to unstable training as the most recent episodes are likely to be correlated. To solve this, we can use a replay buffer (or an experience replay) that stores the past transitions of the form\n"," $(s, a, r, s')$.\n","\n"," Briefly, instead of learning directly from the most recent experience, the agent samples a batch of experiences uniformly from this buffer. This mechanism helps to break correlations between consecutive samples and improves the stability of learning. By reusing past transitions multiple times, the agent can also learn more efficiently from limited interaction with the environment, which is especially important when data collection is costly.\n"],"metadata":{"id":"k9A1qiNeruSe"}},{"cell_type":"code","source":["#!pip install gymnax"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"C72TS_XTlAy9","executionInfo":{"status":"ok","timestamp":1756715126146,"user_tz":-480,"elapsed":9972,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"cd8360d9-6e55-46f0-9d60-9e177bd384db"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnax\n","  Downloading gymnax-0.0.9-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from gymnax) (0.5.3)\n","Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from gymnax) (0.10.6)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from gymnax) (1.2.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from gymnax) (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from gymnax) (0.13.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (2.0.2)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (1.1.1)\n","Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.2.5)\n","Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.11.23)\n","Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.1.76)\n","Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (13.9.4)\n","Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (4.15.0)\n","Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (6.0.2)\n","Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->gymnax) (0.1.10)\n","Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax->gymnax) (0.5.3)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->gymnax) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->gymnax) (3.4.0)\n","Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->gymnax) (1.16.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->gymnax) (3.1.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->gymnax) (0.0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gymnax) (2.9.0.post0)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->gymnax) (2.2.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->gymnax) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->gymnax) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->gymnax) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->gymnax) (2.19.2)\n","Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax->flax->gymnax) (1.4.0)\n","Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax->gymnax) (0.1.90)\n","Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (1.13.0)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (1.6.0)\n","Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (24.1.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (5.29.5)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (4.13.0)\n","Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax->gymnax) (3.20.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax->gymnax) (75.2.0)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax->gymnax) (0.12.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->gymnax) (0.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax) (3.23.0)\n","Downloading gymnax-0.0.9-py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gymnax\n","Successfully installed gymnax-0.0.9\n"]}]},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","import optax\n","import gymnax\n","from gymnax.wrappers.purerl import FlattenObservationWrapper\n","import jax.tree_util as jtu"],"metadata":{"id":"bAm6a3GXgAey","executionInfo":{"status":"ok","timestamp":1756715139553,"user_tz":-480,"elapsed":2067,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"H34FsCCAdhQA","executionInfo":{"status":"ok","timestamp":1756715145862,"user_tz":-480,"elapsed":4,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, capacity, obs_dim, n_envs):\n","        self.capacity = capacity\n","        self.n_envs = n_envs\n","        self.ptr = 0\n","        self.size = 0   # number of valid samples\n","\n","        self.obs_buf = jnp.zeros((capacity, obs_dim))\n","        self.actions_buf = jnp.zeros((capacity,), dtype=jnp.int32)\n","        self.rewards_buf = jnp.zeros((capacity,))\n","        self.next_obs_buf = jnp.zeros((capacity, obs_dim))\n","        self.dones_buf = jnp.zeros((capacity,))\n","\n","    def add(self, obs, actions, rewards, next_obs, dones):\n","        \"\"\"\n","        Add a batch of transitions from all environments.\n","        obs: (n_envs, obs_dim)\n","        actions: (n_envs,)\n","        rewards: (n_envs,)\n","        next_obs: (n_envs, obs_dim)\n","        dones: (n_envs,)\n","        \"\"\"\n","        n = obs.shape[0]\n","        idxs = (jnp.arange(n) + self.ptr) % self.capacity\n","\n","        self.obs_buf = self.obs_buf.at[idxs].set(obs)\n","        self.actions_buf = self.actions_buf.at[idxs].set(actions)\n","        self.rewards_buf = self.rewards_buf.at[idxs].set(rewards)\n","        self.next_obs_buf = self.next_obs_buf.at[idxs].set(next_obs)\n","        self.dones_buf = self.dones_buf.at[idxs].set(dones)\n","\n","        # advance pointer\n","        self.ptr = (self.ptr + n) % self.capacity\n","        # track current size\n","        self.size = min(self.size + n, self.capacity)\n","\n","    def sample(self, rng, batch_size):\n","        max_size = self.size  # only sample from valid entries\n","        idxs = jax.random.randint(rng, (batch_size,), 0, max_size)\n","        return (self.obs_buf[idxs],\n","                self.actions_buf[idxs],\n","                self.rewards_buf[idxs],\n","                self.next_obs_buf[idxs],\n","                self.dones_buf[idxs])\n","\n","    def __len__(self):\n","        return self.size\n"]},{"cell_type":"code","source":["# Wrap the environment to ensure consistent observation shapes\n","env, env_params = gymnax.make(\"CartPole-v1\")\n","env = FlattenObservationWrapper(env)\n","env_params = env_params\n","\n","def rbf_features(x, centers, sigma=0.5):\n","    # x: (d,) or (batch_size, d), centers: (n_centers, d)\n","\n","    # Normalize input to appropriate range for CartPole\n","    x= x / jnp.array([2.4, 3.0, 0.2, 3.0])  # CartPole observation scaling\n","\n","    diffs = x[None] - centers if x.ndim == 1 else x[:, None] - centers\n","    sq_dist = jnp.sum(diffs**2, axis=-1)\n","    return jnp.exp(-sq_dist / (2 * sigma**2))\n","\n","def init_params(rng, n_features, n_actions):\n","    W = jax.random.normal(rng, (n_features, n_actions)) * 0.1\n","    return W\n","\n","def q_values(W, obs, centers, sigma=0.5):\n","    phi = rbf_features(obs, centers, sigma)  # (batch_size, n_features) or (n_features,)\n","    return jnp.dot(phi, W)  # (batch_size, n_actions) or (n_actions,)\n","\n","def select_action(W, obs, rng, centers, sigma=0.5, epsilon=0.1):\n","    q = q_values(W, obs, centers, sigma)\n","    greedy = jnp.argmax(q, axis=-1)\n","    explore = jax.random.bernoulli(rng, epsilon, shape=greedy.shape)\n","    random_actions = jax.random.randint(rng, greedy.shape, 0, q.shape[-1])\n","    return jnp.where(explore, random_actions, greedy)\n","\n","def td_loss(W, obs, action, reward, next_obs, done, gamma, centers, sigma):\n","    q = q_values(W, obs, centers, sigma)\n","    q_selected = jnp.take_along_axis(q, action[:, None], axis=-1).squeeze()\n","\n","    next_q = jnp.max(q_values(W, next_obs, centers, sigma), axis=-1)\n","    target = reward + gamma * (1 - done) * next_q\n","\n","    return jnp.mean(0.5 * (q_selected - target) ** 2)\n","\n","def train_parallel_simple(num_episodes=500, lr=1e-2, gamma=0.99, n_centers=50,\n","                         sigma=0.5, num_envs=16, batch_size=16):\n","    obs_dim = env.observation_space(env_params).shape[0]\n","    n_actions = env.action_space(env_params).n\n","\n","    rng = jax.random.PRNGKey(0)\n","    rng, centers_rng, init_rng = jax.random.split(rng, 3)\n","\n","    # Random RBF centers\n","    centers = jax.random.uniform(centers_rng, (n_centers, obs_dim), minval=-1, maxval=1)\n","    W = init_params(init_rng, n_centers, n_actions)\n","\n","    opt = optax.adam(lr)\n","    opt_state = opt.init(W)\n","\n","    # Vectorized functions\n","    vmap_reset = jax.vmap(env.reset, in_axes=(0, None))\n","    vmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n","    vmap_select_action = jax.vmap(select_action, in_axes=(None, 0, 0, None, None, None))\n","\n","    @jax.jit\n","    def update_batch(W, opt_state, obs, actions, rewards, next_obs, dones):\n","        def batch_loss(W):\n","            return td_loss(W, obs, actions, rewards, next_obs, dones, gamma, centers, sigma)\n","\n","        grads = jax.grad(batch_loss)(W)\n","        updates, opt_state = opt.update(grads, opt_state, W)\n","        W = optax.apply_updates(W, updates)\n","        return W, opt_state\n","\n","    # Initialize environments\n","    rng, *env_rngs = jax.random.split(rng, num_envs + 1)\n","    obs, states = vmap_reset(jnp.array(env_rngs), env_params)\n","\n","    episode_rewards = jnp.zeros(num_envs)\n","    all_rewards = []\n","\n","    buffer = ReplayBuffer(capacity=100000, obs_dim=obs_dim, n_envs=num_envs)   # initialize buffer\n","\n","    for step in range(100000):  # Large enough to collect required episodes\n","        rng, action_rng = jax.random.split(rng)\n","        action_rngs = jax.random.split(action_rng, num_envs)\n","\n","        # Select actions for all environments\n","        actions = vmap_select_action(W, obs, jnp.array(action_rngs), centers, sigma, epsilon)\n","\n","        # Step all environments\n","        rng, *step_rngs = jax.random.split(rng, num_envs + 1)\n","        next_obs, next_states, rewards, dones, _ = vmap_step(\n","            jnp.array(step_rngs), states, actions, env_params\n","        )\n","\n","        # Store experience in replay buffer\n","        buffer.add(obs, actions, rewards, next_obs, dones.astype(jnp.float32))\n","\n","        # Update episode rewards\n","        episode_rewards += rewards\n","\n","        # Record completed episodes and reset counters for done envs\n","        completed_mask = dones.astype(bool)\n","        completed_rewards = episode_rewards[completed_mask]\n","        all_rewards.extend(completed_rewards.tolist())\n","\n","        episode_rewards = episode_rewards * (1 - dones.astype(jnp.float32))\n","\n","        # Compute resets for all (unconditionally for simplicity)\n","        rng, reset_rng = jax.random.split(rng)\n","        reset_rngs = jax.random.split(reset_rng, num_envs)\n","        reset_obs, reset_states = vmap_reset(jnp.array(reset_rngs), env_params)\n","\n","        # Apply resets only to done environments\n","        obs = jnp.where(dones[:, None], reset_obs, next_obs)\n","        states = jtu.tree_map(lambda r, n: jnp.where(dones, r, n), reset_states, next_states)\n","\n","        # Train only if buffer is warm enough\n","        if buffer.size >= 1000:\n","            rng, sample_rng = jax.random.split(rng)\n","            batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = \\\n","                buffer.sample(sample_rng, batch_size)\n","\n","            W, opt_state = update_batch(\n","                W, opt_state,\n","                batch_obs, batch_actions, batch_rewards,\n","                batch_next_obs, batch_dones\n","            )\n","\n","        # Print progress\n","        if step % 100 == 0:\n","            if all_rewards:\n","                recent_rewards = all_rewards[-100:] if len(all_rewards) > 100 else all_rewards\n","                avg_reward = jnp.mean(jnp.array(recent_rewards))\n","                print(f\"Step {step}, Avg reward: {avg_reward:.1f}, \"\n","                      f\"Total episodes: {len(all_rewards)}\")\n","\n","                # Early stopping if solved\n","                if len(all_rewards) >= 100 and avg_reward >= 475.0:\n","                    print(\"CartPole solved!\")\n","                    break\n","            else:\n","                print(f\"Step {step}, No episodes completed yet\")\n","\n","        # Stop if we've collected enough episodes\n","        if len(all_rewards) >= num_episodes:\n","            break\n","\n","    return W, centers, all_rewards\n","\n","# Run training\n","W, centers, rewards = train_parallel_simple(num_episodes=50000, num_envs=16, n_centers=500, lr=5e-3, batch_size=32 )\n","print(f\"Final average reward: {jnp.mean(jnp.array(rewards[-100:])):.1f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4V-PgLLsfwzs","executionInfo":{"status":"error","timestamp":1756703353168,"user_tz":-480,"elapsed":452178,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"eac9f0b9-c928-4750-eb1d-d87340cd8a2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0, No episodes completed yet\n","Step 100, Avg reward: 10.2, Total episodes: 153\n","Step 200, Avg reward: 17.7, Total episodes: 222\n","Step 300, Avg reward: 27.6, Total episodes: 258\n","Step 400, Avg reward: 39.0, Total episodes: 289\n","Step 500, Avg reward: 47.0, Total episodes: 314\n","Step 600, Avg reward: 53.7, Total episodes: 336\n","Step 700, Avg reward: 62.2, Total episodes: 353\n","Step 800, Avg reward: 68.0, Total episodes: 363\n","Step 900, Avg reward: 78.6, Total episodes: 376\n","Step 1000, Avg reward: 91.8, Total episodes: 391\n","Step 1100, Avg reward: 95.4, Total episodes: 396\n","Step 1200, Avg reward: 109.4, Total episodes: 410\n","Step 1300, Avg reward: 113.2, Total episodes: 414\n","Step 1400, Avg reward: 128.4, Total episodes: 425\n","Step 1500, Avg reward: 137.0, Total episodes: 433\n","Step 1600, Avg reward: 146.5, Total episodes: 440\n","Step 1700, Avg reward: 155.3, Total episodes: 447\n","Step 1800, Avg reward: 159.7, Total episodes: 450\n","Step 1900, Avg reward: 173.0, Total episodes: 458\n","Step 2000, Avg reward: 185.1, Total episodes: 464\n","Step 2100, Avg reward: 189.1, Total episodes: 466\n","Step 2200, Avg reward: 200.8, Total episodes: 475\n","Step 2300, Avg reward: 209.3, Total episodes: 479\n","Step 2400, Avg reward: 215.7, Total episodes: 484\n","Step 2500, Avg reward: 222.8, Total episodes: 490\n","Step 2600, Avg reward: 229.7, Total episodes: 493\n","Step 2700, Avg reward: 240.1, Total episodes: 499\n","Step 2800, Avg reward: 250.4, Total episodes: 506\n","Step 2900, Avg reward: 253.2, Total episodes: 509\n","Step 3000, Avg reward: 258.4, Total episodes: 511\n","Step 3100, Avg reward: 271.5, Total episodes: 519\n","Step 3200, Avg reward: 275.9, Total episodes: 523\n","Step 3300, Avg reward: 285.8, Total episodes: 527\n","Step 3400, Avg reward: 292.5, Total episodes: 532\n","Step 3500, Avg reward: 293.9, Total episodes: 534\n","Step 3600, Avg reward: 304.2, Total episodes: 539\n","Step 3700, Avg reward: 310.8, Total episodes: 544\n","Step 3800, Avg reward: 320.1, Total episodes: 550\n","Step 3900, Avg reward: 323.4, Total episodes: 553\n","Step 4000, Avg reward: 325.7, Total episodes: 555\n","Step 4100, Avg reward: 329.4, Total episodes: 557\n","Step 4200, Avg reward: 337.6, Total episodes: 564\n","Step 4300, Avg reward: 347.7, Total episodes: 570\n","Step 4400, Avg reward: 347.7, Total episodes: 570\n","Step 4500, Avg reward: 350.1, Total episodes: 572\n","Step 4600, Avg reward: 357.9, Total episodes: 577\n","Step 4700, Avg reward: 370.2, Total episodes: 585\n","Step 4800, Avg reward: 372.0, Total episodes: 586\n","Step 4900, Avg reward: 371.3, Total episodes: 589\n","Step 5000, Avg reward: 376.2, Total episodes: 596\n","Step 5100, Avg reward: 377.3, Total episodes: 599\n","Step 5200, Avg reward: 384.9, Total episodes: 605\n","Step 5300, Avg reward: 381.1, Total episodes: 607\n","Step 5400, Avg reward: 383.9, Total episodes: 610\n","Step 5500, Avg reward: 389.3, Total episodes: 613\n","Step 5600, Avg reward: 394.4, Total episodes: 618\n","Step 5700, Avg reward: 400.1, Total episodes: 624\n","Step 5800, Avg reward: 399.3, Total episodes: 627\n","Step 5900, Avg reward: 399.5, Total episodes: 628\n","Step 6000, Avg reward: 405.9, Total episodes: 633\n","Step 6100, Avg reward: 409.4, Total episodes: 637\n","Step 6200, Avg reward: 412.9, Total episodes: 642\n","Step 6300, Avg reward: 413.1, Total episodes: 643\n","Step 6400, Avg reward: 409.4, Total episodes: 646\n","Step 6500, Avg reward: 416.0, Total episodes: 651\n","Step 6600, Avg reward: 418.3, Total episodes: 657\n","Step 6700, Avg reward: 416.6, Total episodes: 662\n","Step 6800, Avg reward: 411.7, Total episodes: 665\n","Step 6900, Avg reward: 408.0, Total episodes: 668\n","Step 7000, Avg reward: 406.5, Total episodes: 675\n","Step 7100, Avg reward: 406.5, Total episodes: 675\n","Step 7200, Avg reward: 410.1, Total episodes: 679\n","Step 7300, Avg reward: 407.6, Total episodes: 684\n","Step 7400, Avg reward: 407.8, Total episodes: 686\n","Step 7500, Avg reward: 412.2, Total episodes: 693\n","Step 7600, Avg reward: 413.4, Total episodes: 695\n","Step 7700, Avg reward: 414.1, Total episodes: 699\n","Step 7800, Avg reward: 413.0, Total episodes: 703\n","Step 7900, Avg reward: 419.1, Total episodes: 707\n","Step 8000, Avg reward: 415.9, Total episodes: 713\n","Step 8100, Avg reward: 415.7, Total episodes: 715\n","Step 8200, Avg reward: 412.1, Total episodes: 718\n","Step 8300, Avg reward: 413.1, Total episodes: 722\n","Step 8400, Avg reward: 413.1, Total episodes: 724\n","Step 8500, Avg reward: 417.7, Total episodes: 732\n","Step 8600, Avg reward: 412.9, Total episodes: 735\n","Step 8700, Avg reward: 411.1, Total episodes: 740\n","Step 8800, Avg reward: 407.1, Total episodes: 743\n","Step 8900, Avg reward: 404.4, Total episodes: 747\n","Step 9000, Avg reward: 404.0, Total episodes: 750\n","Step 9100, Avg reward: 400.4, Total episodes: 755\n","Step 9200, Avg reward: 402.9, Total episodes: 761\n","Step 9300, Avg reward: 405.4, Total episodes: 764\n","Step 9400, Avg reward: 406.0, Total episodes: 768\n","Step 9500, Avg reward: 402.8, Total episodes: 771\n","Step 9600, Avg reward: 400.2, Total episodes: 776\n","Step 9700, Avg reward: 401.4, Total episodes: 780\n","Step 9800, Avg reward: 399.9, Total episodes: 785\n","Step 9900, Avg reward: 394.7, Total episodes: 790\n","Step 10000, Avg reward: 394.4, Total episodes: 793\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-435138128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# Run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_parallel_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final average reward: {jnp.mean(jnp.array(rewards[-100:])):.1f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-435138128.py\u001b[0m in \u001b[0;36mtrain_parallel_simple\u001b[0;34m(num_episodes, lr, gamma, n_centers, sigma, num_envs, batch_size)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mall_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompleted_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Compute resets for all (unconditionally for simplicity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/ufunc_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"where argument of {self}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__static_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'call'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_vectorized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## Epsilon Decay\n","So far, we have been keeping $\\epsilon$ to be fixed (0.1 in the above case). However, we know that while exploration is essential for an agent to discover rewarding states and actions, maintaining a high level of exploration throughout training can prevent the agent from fully exploiting the knowledge it has acquired. To balance exploration and exploitation, we can gradually reduce the probability of taking random actions over time using an exponential decay schedule. Exponential decay decreases the exploration rate rapidly at the beginning when the agent knows very little about the environment, and then more slowly as training progresses. This allows the agent to explore widely in the early stages, while gradually shifting towards exploiting learned strategies once sufficient experience has been gathered.\n","\n","Using exponential decay for exploration helps improve learning stability and efficiency. By reducing random actions in a controlled manner, the agent avoids being trapped in suboptimal behaviors caused by excessive exploration, while still retaining some chance to discover better actions later in training. This is especially true if the agent has almost \"fully learned\", in that case, we do not want the agent to explore as much.\n"],"metadata":{"id":"k_W-N2m1biUc"}},{"cell_type":"code","source":["# Wrap the environment to ensure consistent observation shapes\n","env, env_params = gymnax.make(\"CartPole-v1\")\n","env = FlattenObservationWrapper(env)\n","env_params = env_params\n","\n","def rbf_features(x, centers, sigma=0.5):\n","    # x: (d,) or (batch_size, d), centers: (n_centers, d)\n","\n","    # Normalize input to appropriate range for CartPole\n","    x= x / jnp.array([2.4, 3.0, 0.2, 3.0])  # CartPole observation scaling\n","\n","    diffs = x[None] - centers if x.ndim == 1 else x[:, None] - centers\n","    sq_dist = jnp.sum(diffs**2, axis=-1)\n","    return jnp.exp(-sq_dist / (2 * sigma**2))\n","\n","def init_params(rng, n_features, n_actions):\n","    W = jax.random.normal(rng, (n_features, n_actions)) * 0.1\n","    return W\n","\n","def q_values(W, obs, centers, sigma=0.5):\n","    phi = rbf_features(obs, centers, sigma)  # (batch_size, n_features) or (n_features,)\n","    return jnp.dot(phi, W)  # (batch_size, n_actions) or (n_actions,)\n","\n","def select_action(W, obs, rng, centers, sigma=0.5, epsilon=0.1):\n","    q = q_values(W, obs, centers, sigma)\n","    greedy = jnp.argmax(q, axis=-1)\n","    explore = jax.random.bernoulli(rng, epsilon, shape=greedy.shape)\n","    random_actions = jax.random.randint(rng, greedy.shape, 0, q.shape[-1])\n","    return jnp.where(explore, random_actions, greedy)\n","\n","def td_loss(W, obs, action, reward, next_obs, done, gamma, centers, sigma):\n","    q = q_values(W, obs, centers, sigma)\n","    q_selected = jnp.take_along_axis(q, action[:, None], axis=-1).squeeze()\n","\n","    next_q = jnp.max(q_values(W, next_obs, centers, sigma), axis=-1)\n","    target = reward + gamma * (1 - done) * next_q\n","\n","    return jnp.mean(0.5 * (q_selected - target) ** 2)\n","\n","def epsilon_schedule(step, eps_start=1.0, eps_end=0.05, decay_rate=0.999):\n","    return jnp.maximum(eps_end, eps_start * (decay_rate ** step))\n","\n","def train_parallel_simple(num_episodes=500, lr=1e-2, gamma=0.99, n_centers=50,\n","                         sigma=0.5, num_envs=16, batch_size=16):\n","    obs_dim = env.observation_space(env_params).shape[0]\n","    n_actions = env.action_space(env_params).n\n","\n","    rng = jax.random.PRNGKey(0)\n","    rng, centers_rng, init_rng = jax.random.split(rng, 3)\n","\n","    # Random RBF centers\n","    centers = jax.random.uniform(centers_rng, (n_centers, obs_dim), minval=-1, maxval=1)\n","    W = init_params(init_rng, n_centers, n_actions)\n","\n","    opt = optax.adam(lr)\n","    opt_state = opt.init(W)\n","\n","    # Vectorized functions\n","    vmap_reset = jax.vmap(env.reset, in_axes=(0, None))\n","    vmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n","    vmap_select_action = jax.vmap(select_action, in_axes=(None, 0, 0, None, None, None))\n","\n","    @jax.jit\n","    def update_batch(W, opt_state, obs, actions, rewards, next_obs, dones):\n","        def batch_loss(W):\n","            return td_loss(W, obs, actions, rewards, next_obs, dones, gamma, centers, sigma)\n","\n","        grads = jax.grad(batch_loss)(W)\n","        updates, opt_state = opt.update(grads, opt_state, W)\n","        W = optax.apply_updates(W, updates)\n","        return W, opt_state\n","\n","    # Initialize environments\n","    rng, *env_rngs = jax.random.split(rng, num_envs + 1)\n","    obs, states = vmap_reset(jnp.array(env_rngs), env_params)\n","\n","    episode_rewards = jnp.zeros(num_envs)\n","    all_rewards = []\n","\n","    buffer = ReplayBuffer(capacity=100000, obs_dim=obs_dim, n_envs=num_envs)   # initialize buffer\n","\n","    for step in range(10000):  # Large enough to collect required episodes\n","        rng, action_rng = jax.random.split(rng)\n","        action_rngs = jax.random.split(action_rng, num_envs)\n","\n","        epsilon = epsilon_schedule(step, eps_start=1.0, eps_end=0.01, decay_rate = 0.999)\n","\n","        # Select actions for all environments\n","        actions = vmap_select_action(W, obs, jnp.array(action_rngs), centers, sigma, epsilon)\n","\n","        # Step all environments\n","        rng, *step_rngs = jax.random.split(rng, num_envs + 1)\n","        next_obs, next_states, rewards, dones, _ = vmap_step(\n","            jnp.array(step_rngs), states, actions, env_params\n","        )\n","\n","        # Store experience in replay buffer\n","        buffer.add(obs, actions, rewards, next_obs, dones.astype(jnp.float32))\n","\n","        # Update episode rewards\n","        episode_rewards += rewards\n","\n","        # Record completed episodes and reset counters for done envs\n","        completed_mask = dones.astype(bool)\n","        completed_rewards = episode_rewards[completed_mask]\n","        all_rewards.extend(completed_rewards.tolist())\n","\n","        episode_rewards = episode_rewards * (1 - dones.astype(jnp.float32))\n","\n","        # Compute resets for all (unconditionally for simplicity)\n","        rng, reset_rng = jax.random.split(rng)\n","        reset_rngs = jax.random.split(reset_rng, num_envs)\n","        reset_obs, reset_states = vmap_reset(jnp.array(reset_rngs), env_params)\n","\n","        # Apply resets only to done environments\n","        obs = jnp.where(dones[:, None], reset_obs, next_obs)\n","        states = jtu.tree_map(lambda r, n: jnp.where(dones, r, n), reset_states, next_states)\n","\n","        # Train only if buffer is warm enough\n","        if buffer.size >= 1000:\n","            rng, sample_rng = jax.random.split(rng)\n","            batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = \\\n","                buffer.sample(sample_rng, batch_size)\n","\n","            W, opt_state = update_batch(\n","                W, opt_state,\n","                batch_obs, batch_actions, batch_rewards,\n","                batch_next_obs, batch_dones\n","            )\n","\n","        # Print progress\n","        if step % 100 == 0:\n","            if all_rewards:\n","                recent_rewards = all_rewards[-100:] if len(all_rewards) > 100 else all_rewards\n","                avg_reward = jnp.mean(jnp.array(recent_rewards))\n","                print(f\"Step {step}, Avg reward: {avg_reward:.1f}, \"\n","                      f\"Total episodes: {len(all_rewards)}\")\n","\n","                # Early stopping if solved\n","                if len(all_rewards) >= 100 and avg_reward >= 475.0:\n","                    print(\"CartPole solved!\")\n","                    break\n","            else:\n","                print(f\"Step {step}, No episodes completed yet\")\n","\n","        # Stop if we've collected enough episodes\n","        if len(all_rewards) >= num_episodes:\n","            break\n","\n","    return W, centers, all_rewards\n","\n","# Run training\n","W, centers, rewards = train_parallel_simple(num_episodes=50000, num_envs=16, n_centers=500, lr=5e-3, batch_size=32 )\n","print(f\"Final average reward: {jnp.mean(jnp.array(rewards[-100:])):.1f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-uK8-kteYK12","executionInfo":{"status":"ok","timestamp":1756715917554,"user_tz":-480,"elapsed":450001,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"4c368e8a-03e4-49e0-b46f-b4e32620b162"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 0, No episodes completed yet\n","Step 100, Avg reward: 19.6, Total episodes: 72\n","Step 200, Avg reward: 21.2, Total episodes: 145\n","Step 300, Avg reward: 23.7, Total episodes: 207\n","Step 400, Avg reward: 26.3, Total episodes: 267\n","Step 500, Avg reward: 27.8, Total episodes: 313\n","Step 600, Avg reward: 32.1, Total episodes: 362\n","Step 700, Avg reward: 34.6, Total episodes: 398\n","Step 800, Avg reward: 39.9, Total episodes: 431\n","Step 900, Avg reward: 46.5, Total episodes: 455\n","Step 1000, Avg reward: 54.4, Total episodes: 481\n","Step 1100, Avg reward: 60.4, Total episodes: 502\n","Step 1200, Avg reward: 67.3, Total episodes: 523\n","Step 1300, Avg reward: 72.9, Total episodes: 536\n","Step 1400, Avg reward: 79.0, Total episodes: 548\n","Step 1500, Avg reward: 83.5, Total episodes: 559\n","Step 1600, Avg reward: 94.6, Total episodes: 570\n","Step 1700, Avg reward: 103.7, Total episodes: 577\n","Step 1800, Avg reward: 110.8, Total episodes: 586\n","Step 1900, Avg reward: 112.6, Total episodes: 588\n","Step 2000, Avg reward: 127.8, Total episodes: 595\n","Step 2100, Avg reward: 147.4, Total episodes: 602\n","Step 2200, Avg reward: 154.6, Total episodes: 609\n","Step 2300, Avg reward: 161.2, Total episodes: 613\n","Step 2400, Avg reward: 173.8, Total episodes: 618\n","Step 2500, Avg reward: 188.8, Total episodes: 626\n","Step 2600, Avg reward: 195.3, Total episodes: 629\n","Step 2700, Avg reward: 211.4, Total episodes: 637\n","Step 2800, Avg reward: 217.2, Total episodes: 640\n","Step 2900, Avg reward: 230.9, Total episodes: 645\n","Step 3000, Avg reward: 242.9, Total episodes: 651\n","Step 3100, Avg reward: 251.6, Total episodes: 657\n","Step 3200, Avg reward: 259.8, Total episodes: 664\n","Step 3300, Avg reward: 262.0, Total episodes: 666\n","Step 3400, Avg reward: 273.0, Total episodes: 673\n","Step 3500, Avg reward: 279.7, Total episodes: 678\n","Step 3600, Avg reward: 282.0, Total episodes: 681\n","Step 3700, Avg reward: 298.5, Total episodes: 688\n","Step 3800, Avg reward: 299.6, Total episodes: 692\n","Step 3900, Avg reward: 298.6, Total episodes: 696\n","Step 4000, Avg reward: 298.4, Total episodes: 702\n","Step 4100, Avg reward: 301.9, Total episodes: 707\n","Step 4200, Avg reward: 308.4, Total episodes: 713\n","Step 4300, Avg reward: 309.3, Total episodes: 718\n","Step 4400, Avg reward: 310.3, Total episodes: 720\n","Step 4500, Avg reward: 316.1, Total episodes: 726\n","Step 4600, Avg reward: 318.9, Total episodes: 731\n","Step 4700, Avg reward: 319.1, Total episodes: 736\n","Step 4800, Avg reward: 323.5, Total episodes: 739\n","Step 4900, Avg reward: 321.0, Total episodes: 744\n","Step 5000, Avg reward: 324.9, Total episodes: 750\n","Step 5100, Avg reward: 326.8, Total episodes: 753\n","Step 5200, Avg reward: 326.7, Total episodes: 761\n","Step 5300, Avg reward: 327.0, Total episodes: 764\n","Step 5400, Avg reward: 327.4, Total episodes: 767\n","Step 5500, Avg reward: 329.9, Total episodes: 776\n","Step 5600, Avg reward: 330.2, Total episodes: 780\n","Step 5700, Avg reward: 331.8, Total episodes: 783\n","Step 5800, Avg reward: 329.4, Total episodes: 790\n","Step 5900, Avg reward: 329.4, Total episodes: 791\n","Step 6000, Avg reward: 333.9, Total episodes: 796\n","Step 6100, Avg reward: 334.9, Total episodes: 803\n","Step 6200, Avg reward: 335.9, Total episodes: 805\n","Step 6300, Avg reward: 338.2, Total episodes: 811\n","Step 6400, Avg reward: 337.0, Total episodes: 817\n","Step 6500, Avg reward: 339.1, Total episodes: 820\n","Step 6600, Avg reward: 336.3, Total episodes: 827\n","Step 6700, Avg reward: 335.9, Total episodes: 831\n","Step 6800, Avg reward: 336.4, Total episodes: 835\n","Step 6900, Avg reward: 330.3, Total episodes: 844\n","Step 7000, Avg reward: 332.8, Total episodes: 846\n","Step 7100, Avg reward: 327.6, Total episodes: 851\n","Step 7200, Avg reward: 327.3, Total episodes: 859\n","Step 7300, Avg reward: 327.3, Total episodes: 859\n","Step 7400, Avg reward: 329.8, Total episodes: 864\n","Step 7500, Avg reward: 332.8, Total episodes: 874\n","Step 7600, Avg reward: 333.1, Total episodes: 875\n","Step 7700, Avg reward: 331.9, Total episodes: 879\n","Step 7800, Avg reward: 330.5, Total episodes: 886\n","Step 7900, Avg reward: 332.9, Total episodes: 891\n","Step 8000, Avg reward: 332.6, Total episodes: 893\n","Step 8100, Avg reward: 329.9, Total episodes: 896\n","Step 8200, Avg reward: 332.1, Total episodes: 901\n","Step 8300, Avg reward: 334.9, Total episodes: 908\n","Step 8400, Avg reward: 335.5, Total episodes: 910\n","Step 8500, Avg reward: 338.2, Total episodes: 917\n","Step 8600, Avg reward: 336.8, Total episodes: 919\n","Step 8700, Avg reward: 340.8, Total episodes: 925\n","Step 8800, Avg reward: 339.2, Total episodes: 931\n","Step 8900, Avg reward: 341.5, Total episodes: 934\n","Step 9000, Avg reward: 340.1, Total episodes: 939\n","Step 9100, Avg reward: 340.6, Total episodes: 945\n","Step 9200, Avg reward: 342.1, Total episodes: 948\n","Step 9300, Avg reward: 346.4, Total episodes: 952\n","Step 9400, Avg reward: 347.8, Total episodes: 955\n","Step 9500, Avg reward: 351.9, Total episodes: 962\n","Step 9600, Avg reward: 354.4, Total episodes: 965\n","Step 9700, Avg reward: 355.1, Total episodes: 968\n","Step 9800, Avg reward: 355.9, Total episodes: 971\n","Step 9900, Avg reward: 358.9, Total episodes: 976\n","Final average reward: 364.1\n"]}]}]}