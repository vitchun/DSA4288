{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMuQ2W5C6GVTCFTjAmGzJSX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Bayesian Logistic Regression\n","\n","In binary logistic regression, we model the probability of a binary outcome $y \\in \\{0,1\\}$ given an input feature vector $x \\in \\mathbb{R}^d$ as\n","\n","$$\n","p(y = 1 \\mid x, w) = \\sigma(w^\\top x),\n","$$\n","\n","where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function, and $w$ denotes the weight vector.  \n","For a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$, the likelihood of the data under the model is\n","\n","$$\n","p(y_{1:N} \\mid X, w)\n","= \\prod_{i=1}^N \\sigma(w^\\top x_i)^{y_i} [1 - \\sigma(w^\\top x_i)]^{1 - y_i}.\n","$$\n","\n","---\n","\n","## Bayesian Formulation\n","\n","In the Bayesian setting, the model parameters $w$ are treated as random variables with a prior distribution $p(w)$.  \n","Given observed data $(X, y)$, we infer a posterior distribution via Bayes’ theorem:\n","\n","$$\n","p(w \\mid X, y) = \\frac{p(y \\mid X, w) \\, p(w)}{p(y \\mid X)},\n","$$\n","\n","where the denominator $p(y \\mid X) = \\int p(y \\mid X, w) p(w) \\, dw$ is the marginal likelihood (or evidence).  \n","This term is typically intractable for logistic regression due to the non-conjugacy between the sigmoid likelihood and the Gaussian prior.\n","\n","---\n","\n","## Prior Specification\n","\n","A common choice for the prior is an isotropic Gaussian:\n","\n","$$\n","p(w) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I),\n","$$\n","\n","where $\\alpha$ denotes the prior precision (inverse variance).  \n","The corresponding log-prior is thus\n","\n","$$\n","\\log p(w) = -\\frac{\\alpha}{2} w^\\top w + \\text{const.}\n","$$\n","\n","---\n","\n","## Posterior Distribution\n","\n","Combining the likelihood and prior gives the unnormalized log posterior:\n","\n","$$\n","\\log \\overline{\\pi}(w)\n","= \\sum_{i=1}^N \\Big[ y_i \\log \\sigma(w^\\top x_i)\n","+ (1 - y_i) \\log (1 - \\sigma(w^\\top x_i)) \\Big]\n","- \\frac{\\alpha}{2} w^\\top w.\n","$$\n","\n","Thus, the posterior is given by\n","\n","$$\n","p(w \\mid X, y) \\propto \\exp\\big( \\log \\overline{\\pi}(w) \\big).\n","$$\n","\n","To obtain a normalized posterior, we would need to compute:\n","\n","$$\n","p(w \\mid X, y)\n","= \\frac{\\overline{\\pi}(w)}{\\int \\overline{\\pi}(w) \\, dw}.\n","$$\n","\n","The denominator, which integrates over all possible values of $w$, is called the marginal likelihood or evidence:\n","\n","$$\n","p(y \\mid X)\n","= \\int p(y \\mid X, w) \\, p(w) \\, dw.\n","$$\n","\n","The problem arises because the likelihood term $p(y \\mid X, w)$ involves the sigmoid function $\\sigma(w^\\top x)$, making the integrand non-Gaussian and analytically intractable.  \n","In other words, there is no closed-form solution for this integral since the sigmoid function does not combine neatly with the Gaussian prior.\n","\n","---\n","\n","## Approximate Inference\n","\n","Since $p(y \\mid X) = \\int p(y \\mid X, w) p(w)\\,dw$ has no closed form, several approximate methods are commonly used:\n","\n","- Laplace approximation:\n","  - Approximate the posterior by a Gaussian centered at the mode $w_{\\text{MAP}}$:\n","  $$\n","  p(w \\mid X, y) \\approx \\mathcal{N}(w_{\\text{MAP}}, H^{-1}),\n","  $$\n","  where $H = -\\nabla^2 \\log p(w \\mid X, y)$ is the Hessian evaluated at $w_{\\text{MAP}}$.  \n","  This provides a simple, local approximation but may be inaccurate for highly non-Gaussian or multimodal posteriors.\n","\n","- Markov Chain Monte Carlo (MCMC):\n","  - Sampling-based methods construct a Markov chain whose stationary distribution is $p(w \\mid X, y)$.  \n","  By iteratively proposing and accepting parameter updates, these methods generate approximate samples $w^{(1)}, w^{(2)}, \\ldots \\sim p(w \\mid X, y)$.  \n","  Popular variants include Metropolis–Hastings, Langevin dynamics, and Hamiltonian Monte Carlo, which differ in how proposals are generated and how gradient information is used to guide sampling.\n","\n","---\n","\n","## Predictive Distribution\n","\n","After obtaining samples from the posterior, the predictive probability for a new data point $x_*$ is computed by marginalizing over the posterior:\n","\n","$$\n","p(y_* = 1 \\mid x_*, X, y)\n","= \\int \\sigma(w^\\top x_*) \\, p(w \\mid X, y) \\, dw.\n","$$\n","\n","In practice, this integral is approximated using Monte Carlo samples from the posterior:\n","\n","$$\n","p(y_* = 1 \\mid x_*, X, y)\n","\\approx \\frac{1}{M} \\sum_{m=1}^M \\sigma\\big( w^{(m)\\top} x_* \\big),\n","$$\n","\n","where $\\{w^{(m)}\\}_{m=1}^M$ are samples drawn from $p(w \\mid X, y)$.\n","\n","---\n","\n","## Connection to Sampling and RLFS\n","\n","Instead of using Laplace Approximations / MCMC, we will attempt to use reinforcement learning for sampling to this problem.\n","\n"],"metadata":{"id":"Fj6lFvPMF3Xb"}},{"cell_type":"markdown","source":["# Direct Backpropagation\n"],"metadata":{"id":"AAhpHwbwdVwR"}},{"cell_type":"code","source":["# ===================== Differentiable RLFS on Breast Cancer Dataset =====================\n","# Direct optimization (no actor-critic) of RLFS objective for Bayesian Logistic Regression.\n","# -----------------------------------------------------------------------------------------\n","import jax\n","import jax.numpy as jnp\n","from jax import random, lax, value_and_grad, jit\n","from flax import linen as nn\n","import optax\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tqdm import trange\n","\n","print(\"Devices:\", jax.devices())\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Dataset: Breast Cancer Wisconsin Diagnostic\n","# ---------------------------------------------------------------------------------------------------\n","data = load_breast_cancer()\n","X = data.data\n","y = data.target.astype(np.float32)\n","\n","# Standardize features\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Train/Test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","X_train = jnp.array(X_train)\n","X_test  = jnp.array(X_test)\n","y_train = jnp.array(y_train)\n","y_test  = jnp.array(y_test)\n","\n","D_base = X_train.shape[1]  # 30\n","D = D_base + 1             # weights + bias\n","print(f\"Dataset: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples, dim={D_base}\")\n","\n","# ---------------------------------------------------------------------------------------------------\n","# BLR target (weights + bias concatenated)\n","# ---------------------------------------------------------------------------------------------------\n","def blr_log_unnormalized(params, X, y, alpha=1.0):\n","    w, b = params[:-1], params[-1]\n","    logits = X @ w + b\n","    log_lik = jnp.sum(y * jax.nn.log_sigmoid(logits) + (1 - y) * jax.nn.log_sigmoid(-logits))\n","    log_prior = -0.5 * alpha * jnp.dot(params, params)\n","    return log_lik + log_prior\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Actor (policy) network\n","# ---------------------------------------------------------------------------------------------------\n","class TimeEmbed(nn.Module):\n","    hidden: int = 64\n","    @nn.compact\n","    def __call__(self, t):\n","        freqs = jnp.asarray([1., 2., 4., 8., 16.])\n","        sinus = jnp.concatenate([\n","            jnp.sin(2*jnp.pi*freqs[None,:]*t[:,None]),\n","            jnp.cos(2*jnp.pi*freqs[None,:]*t[:,None])\n","        ], axis=-1)\n","        h = nn.relu(nn.Dense(self.hidden)(sinus))\n","        return nn.relu(nn.Dense(self.hidden)(h))\n","\n","class Actor(nn.Module):\n","    hidden: int = 256\n","    out_dim: int = 31  # D = 30 + 1\n","    @nn.compact\n","    def __call__(self, x, t):\n","        te = TimeEmbed()(t)\n","        h = jnp.concatenate([x, te], axis=-1)\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        return 1.5 * jnp.tanh(nn.Dense(self.out_dim)(h))\n","\n","# ---------------------------------------------------------------------------------------------------\n","# RLFS dynamics & log-probs\n","# ---------------------------------------------------------------------------------------------------\n","@jit\n","def logF(x, x_next, a, sigma):\n","    diff = x_next - x - a\n","    return -jnp.sum(diff**2, axis=-1) / (2*sigma**2)\n","\n","@jit\n","def logB(x, x_next, sigma):\n","    diff = x - jnp.sqrt(1-sigma**2)*x_next\n","    return -jnp.sum(diff**2, axis=-1) / (2*sigma**2)\n","\n","def make_rollout_trajectory(T, actor_forward):\n","    invT = jnp.array(1.0/T, dtype=jnp.float32)\n","    def step(carry, _):\n","        key, x, t, params, sigma = carry\n","        key, sub = random.split(key)\n","        a = actor_forward(params, x, t)\n","        eps = random.normal(sub, x.shape)\n","        x_next = jnp.sqrt(1-sigma**2)*x + a + sigma*eps\n","        t_next = t + invT\n","        r = logB(x, x_next, sigma) - logF(x, x_next, a, sigma)\n","        carry = (key, x_next, t_next, params, sigma)\n","        trans = (x, a, t, r, x_next, t_next)\n","        return carry, trans\n","    @jit\n","    def rollout(key, x0, t0, params, sigma):\n","        init = (key, x0, t0, params, sigma)\n","        (key_f, xT, tT, _, _), (xs, as_, ts, rs, xns, tns) = lax.scan(step, init, None, length=T)\n","        return (xs, as_, ts, rs, xns, tns), (xT, tT)\n","    return rollout\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Differentiable RLFS loss\n","# ---------------------------------------------------------------------------------------------------\n","def rlfs_loss(params, key, X, y, alpha, sigma=0.2, T=24, B=512):\n","    actor = Actor(out_dim=D)\n","    rollout = make_rollout_trajectory(T, lambda p, x, t: actor.apply(p, x, t))\n","    key, sub = random.split(key)\n","    x0 = 0.5 * random.normal(sub, (B, D))\n","    t0 = jnp.zeros((B,))\n","    (_, _, _, rs, _, _), (xT, _) = rollout(key, x0, t0, params, sigma)\n","    r_term = jax.vmap(lambda xx: blr_log_unnormalized(xx, X, y, alpha))(xT)\n","    total_r = jnp.sum(rs, axis=0) + r_term\n","    return -jnp.mean(total_r)  # minimize -E[reward]\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Predictive metrics\n","# ---------------------------------------------------------------------------------------------------\n","@jit\n","def predictive_metrics(params_batch, X, y):\n","    w = params_batch[:, :-1]\n","    b = params_batch[:, -1:]\n","    logits = X @ w.T + b.T\n","    probs = jax.nn.sigmoid(logits)\n","    p_mc = jnp.mean(probs, axis=1)\n","    eps = 1e-7\n","    nll = -jnp.mean(y*jnp.log(p_mc+eps)+(1-y)*jnp.log(1-p_mc+eps))\n","    acc = jnp.mean((p_mc>=0.5)==(y>=0.5))\n","    return nll, acc\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Train by backpropagation\n","# ---------------------------------------------------------------------------------------------------\n","LR = 3e-4\n","SIGMA = 0.2\n","T_H = 24\n","ALPHA = 1.0\n","BATCH = 512\n","EPOCHS = 3000\n","\n","actor = Actor(out_dim=D)\n","key = random.PRNGKey(42)\n","params = actor.init(key, jnp.zeros((1, D)), jnp.zeros((1,)))\n","opt = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(LR))\n","opt_state = opt.init(params)\n","\n","@jit\n","def train_step(params, opt_state, key):\n","    loss, grads = value_and_grad(rlfs_loss)(params, key, X_train, y_train, ALPHA, SIGMA, T_H, BATCH)\n","    updates, opt_state = opt.update(grads, opt_state, params)\n","    params = optax.apply_updates(params, updates)\n","    return params, opt_state, loss\n","\n","for it in trange(EPOCHS):\n","    key, sub = random.split(key)\n","    params, opt_state, loss = train_step(params, opt_state, sub)\n","    if it % 100 == 0:\n","        print(f\"[Iter {it:04d}] Loss = {float(loss):.4f}\")\n","\n","# ---------------------------------------------------------------------------------------------------\n","# Evaluate samples\n","# ---------------------------------------------------------------------------------------------------\n","rollout_eval = make_rollout_trajectory(T_H, lambda p, x, t: actor.apply(p, x, t))\n","key, sub = random.split(key)\n","(_, _, _, _, _, _), (xT, _) = rollout_eval(key, 0.5*random.normal(sub, (8000, D)), jnp.zeros((8000,)), params, SIGMA)\n","\n","nll_tr, acc_tr = predictive_metrics(xT, X_train, y_train)\n","nll_te, acc_te = predictive_metrics(xT, X_test,  y_test)\n","print(f\"\\nFinal Train NLL={float(nll_tr):.3f}, Acc={float(acc_tr):.3f}\")\n","print(f\"Final Test  NLL={float(nll_te):.3f}, Acc={float(acc_te):.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vYRXVQHB5Kr","executionInfo":{"status":"ok","timestamp":1761759204557,"user_tz":-480,"elapsed":30571,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"c82c51b0-5882-47f7-c56a-cfe483523599"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Devices: [CudaDevice(id=0)]\n","Dataset: 426 training samples, 143 test samples, dim=30\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 11/3000 [00:05<19:36,  2.54it/s] "]},{"output_type":"stream","name":"stdout","text":["[Iter 0000] Loss = 23740.3320\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 124/3000 [00:06<00:32, 87.32it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0100] Loss = 73.6869\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 222/3000 [00:07<00:21, 127.73it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0200] Loss = 56.4919\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 320/3000 [00:08<00:20, 132.46it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0300] Loss = 53.4176\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 419/3000 [00:09<00:19, 133.97it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0400] Loss = 52.4427\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 519/3000 [00:09<00:18, 134.73it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0500] Loss = 51.7740\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██        | 619/3000 [00:10<00:17, 134.76it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0600] Loss = 51.4914\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 719/3000 [00:11<00:16, 134.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0700] Loss = 51.5036\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 819/3000 [00:12<00:16, 134.64it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0800] Loss = 50.7234\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███       | 919/3000 [00:12<00:15, 134.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0900] Loss = 50.9866\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 1019/3000 [00:13<00:14, 134.64it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1000] Loss = 50.9375\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 1119/3000 [00:14<00:13, 134.63it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1100] Loss = 50.6996\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 1219/3000 [00:15<00:13, 134.79it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1200] Loss = 50.4441\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 1319/3000 [00:15<00:12, 134.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1300] Loss = 50.5180\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 1419/3000 [00:16<00:11, 134.60it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1400] Loss = 50.7294\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████     | 1519/3000 [00:17<00:11, 134.60it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1500] Loss = 50.5651\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 1619/3000 [00:18<00:10, 134.66it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1600] Loss = 50.8842\n"]},{"output_type":"stream","name":"stderr","text":[" 57%|█████▋    | 1719/3000 [00:18<00:09, 134.63it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1700] Loss = 50.4336\n"]},{"output_type":"stream","name":"stderr","text":[" 61%|██████    | 1819/3000 [00:19<00:08, 134.69it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1800] Loss = 50.2907\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 1919/3000 [00:20<00:08, 134.60it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1900] Loss = 50.7628\n"]},{"output_type":"stream","name":"stderr","text":[" 67%|██████▋   | 2019/3000 [00:21<00:07, 134.62it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2000] Loss = 50.4379\n"]},{"output_type":"stream","name":"stderr","text":[" 71%|███████   | 2119/3000 [00:21<00:06, 134.66it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2100] Loss = 50.1321\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 2219/3000 [00:22<00:05, 134.66it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2200] Loss = 50.2747\n"]},{"output_type":"stream","name":"stderr","text":[" 77%|███████▋  | 2319/3000 [00:23<00:05, 134.65it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2300] Loss = 50.2650\n"]},{"output_type":"stream","name":"stderr","text":[" 81%|████████  | 2419/3000 [00:24<00:04, 134.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2400] Loss = 50.3571\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 2519/3000 [00:24<00:03, 134.64it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2500] Loss = 50.1647\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 2619/3000 [00:25<00:02, 134.42it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2600] Loss = 50.4677\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████ | 2719/3000 [00:26<00:02, 134.63it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2700] Loss = 50.4176\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▍| 2819/3000 [00:27<00:01, 134.50it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2800] Loss = 50.4593\n"]},{"output_type":"stream","name":"stderr","text":[" 97%|█████████▋| 2919/3000 [00:27<00:00, 134.89it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2900] Loss = 50.3458\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3000/3000 [00:28<00:00, 105.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Final Train NLL=0.061, Acc=0.988\n","Final Test  NLL=0.085, Acc=0.979\n"]}]},{"cell_type":"markdown","source":["# REINFORCE"],"metadata":{"id":"g4xMrEP0djza"}},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","from jax import random, lax, value_and_grad, jit\n","from flax import linen as nn\n","from flax.training.train_state import TrainState\n","import optax\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tqdm import trange\n","\n","print(\"Devices:\", jax.devices())\n","\n","# =========================================================\n","# 0. Data: Breast Cancer\n","# =========================================================\n","data = load_breast_cancer()\n","X = data.data.astype(np.float32)\n","y = data.target.astype(np.float32)\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X).astype(np.float32)\n","\n","X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n","    X, y, test_size=0.25, random_state=0, stratify=y\n",")\n","X_train = jnp.array(X_train_np)\n","X_test  = jnp.array(X_test_np)\n","y_train = jnp.array(y_train_np)\n","y_test  = jnp.array(y_test_np)\n","\n","D_base = X_train.shape[1]   # 30\n","D = D_base + 1              # 31 (weights + bias)\n","print(f\"Dataset: train={X_train.shape[0]}, test={X_test.shape[0]}, dim={D_base}\")\n","\n","# We'll close over these in jit'd functions\n","ALPHA = 3.0   # BLR prior precision\n","SIGMA = 0.1\n","HORIZON = 32\n","\n","# =========================================================\n","# 1. BLR log unnormalized posterior log π̄(w,b)\n","# =========================================================\n","def blr_log_unnormalized(params, X, y, alpha=1.0):\n","    w, b = params[:-1], params[-1]\n","    logits = X @ w + b\n","    log_lik = jnp.sum(\n","        y * jax.nn.log_sigmoid(logits)\n","        + (1.0 - y) * jax.nn.log_sigmoid(-logits)\n","    )\n","    log_prior = -0.5 * alpha * jnp.dot(params, params)\n","    return log_lik + log_prior\n","\n","# vectorize over batch of params\n","batch_log_unnorm_pi = jit(jax.vmap(\n","    lambda p: blr_log_unnormalized(p, X_train, y_train, ALPHA)\n","))\n","\n","# =========================================================\n","# 2. Nets: TimeEmbed + Actor(μθ)\n","# =========================================================\n","class TimeEmbed(nn.Module):\n","    hidden: int = 64\n","    @nn.compact\n","    def __call__(self, t):  # t: [B]\n","        freqs = jnp.asarray([1., 2., 4., 8., 16.])\n","        sinus = jnp.concatenate([\n","            jnp.sin(2*jnp.pi*freqs[None,:] * t[:,None]),\n","            jnp.cos(2*jnp.pi*freqs[None,:] * t[:,None])\n","        ], axis=-1)    # [B,10]\n","        h = nn.relu(nn.Dense(self.hidden)(sinus))\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        return h       # [B,hidden]\n","\n","class Actor(nn.Module):\n","    out_dim: int\n","    hidden: int = 256\n","    act_scale: float = 1.5\n","    @nn.compact\n","    def __call__(self, x, t):  # x:[B,D], t:[B]\n","        te = TimeEmbed()(t)                     # [B,hidden]\n","        h = jnp.concatenate([x, te], axis=-1)   # [B,D+hidden]\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        a = nn.Dense(self.out_dim)(h)           # [B,D]\n","        return self.act_scale * jnp.tanh(a)     # μθ(x,t)\n","\n","# convenience: applies actor mean\n","def actor_mean(params, x, t):\n","    return Actor(out_dim=D).apply(params, x, t)\n","\n","actor_mean_jit = jit(actor_mean)\n","\n","# =========================================================\n","# 3. RLFS dynamics & per-step terms (JIT-able)\n","# =========================================================\n","\n","@jit\n","def env_step(key, actor_params, x, t, sigma):\n","    \"\"\"\n","    One step for ALL trajectories in batch.\n","    x: [B,D], t: [B]\n","    returns:\n","      key', x_next, t_next, a_sampled, a_det\n","    \"\"\"\n","    a_det = actor_mean_jit(actor_params, x, t)     # [B,D]\n","\n","    key, sub = random.split(key)\n","    eps = random.normal(sub, x.shape)              # [B,D]\n","    a = a_det + sigma * eps                        # [B,D]\n","\n","    x_next = jnp.sqrt(1.0 - sigma**2) * x + a      # [B,D]\n","    t_next = t + (1.0 / HORIZON)                   # [B]\n","\n","    return key, x_next, t_next, a, a_det\n","\n","@jit\n","def logF_consistent(x, x_next, a_det, sigma):\n","    \"\"\"\n","    x_next | x ~ N( mean = sqrt(1-σ²)*x + a_det, cov = σ² I )\n","    returns [B]\n","    \"\"\"\n","    mean = jnp.sqrt(1.0 - sigma**2) * x + a_det\n","    diff = x_next - mean\n","    return -0.5 * jnp.sum((diff**2) / (sigma**2), axis=-1)\n","\n","@jit\n","def logB_consistent(x, x_next, sigma):\n","    \"\"\"\n","    backward kernel: x ~ N( sqrt(1-σ²)*x_next , σ² I )\n","    returns [B]\n","    \"\"\"\n","    mean_back = jnp.sqrt(1.0 - sigma**2) * x_next\n","    diff = x - mean_back\n","    return -0.5 * jnp.sum((diff**2) / (sigma**2), axis=-1)\n","\n","@jit\n","def log_policy_gaussian(actor_params, x, t, a, sigma):\n","    \"\"\"\n","    log πθ(a | x,t) up to const = -0.5 ||(a-μθ)/σ||^2\n","    returns [B]\n","    \"\"\"\n","    mu = actor_mean_jit(actor_params, x, t)  # [B,D]\n","    diff = a - mu\n","    return -0.5 * jnp.sum((diff**2) / (sigma**2), axis=-1)\n","\n","\n","# =========================================================\n","# 4. Rollout *batched* trajectories (JIT, pure functional)\n","#    We'll store everything we need for the policy gradient:\n","#    - xs[t], ts[t], as[t], logpi[t], r[t]\n","#    and final x_T for terminal reward.\n","# =========================================================\n","\n","def rollout_batch_once(key, actor_params, batch_size, sigma):\n","    \"\"\"\n","    Roll out (batch_size) trajectories in parallel for fixed HORIZON.\n","    Returns:\n","      traj dict with shapes:\n","        xs:        [H,B,D]\n","        ts:        [H,B]\n","        acts:      [H,B,D]      (sampled actions)\n","        logp_ts:   [H,B]        (log πθ at those actions)\n","        r_ts:      [H,B]        (per-step flow reward)\n","        x_T:       [B,D]\n","    \"\"\"\n","    # init x0 ~ N(0,0.5^2 I)\n","    key, sub = random.split(key)\n","    x0 = 0.5 * random.normal(sub, (batch_size, D))\n","    t0 = jnp.zeros((batch_size,))\n","\n","    def body(carry, _):\n","        key, x, t = carry\n","        key, x_next, t_next, a_samp, a_det = env_step(key, actor_params, x, t, sigma)\n","        # per-step reward\n","        r_t = logB_consistent(x, x_next, sigma) - logF_consistent(x, x_next, a_det, sigma)\n","        # logπθ(a|x,t)\n","        logp_t = log_policy_gaussian(actor_params, x, t, a_samp, sigma)\n","\n","        new_carry = (key, x_next, t_next)\n","        out_t = (x, t, a_samp, logp_t, r_t, x_next)\n","        return new_carry, out_t\n","\n","    # lax.scan: length = HORIZON\n","    (key_f, x_last, t_last), scan_out = lax.scan(\n","        body,\n","        (key, x0, t0),\n","        xs=None,\n","        length=HORIZON\n","    )\n","    xs, ts, acts, logp_ts, r_ts, xns = scan_out\n","    # xs[t] is x_t at step t, xns[t] is x_{t+1}; last xns[-1] is x_T\n","    x_T = xns[-1]  # [B,D]\n","    return key_f, {\n","        \"xs\": xs,\n","        \"ts\": ts,\n","        \"acts\": acts,\n","        \"logp_ts\": logp_ts,\n","        \"r_ts\": r_ts,\n","        \"x_T\": x_T,\n","    }\n","\n","rollout_batch_once_jit = jit(rollout_batch_once, static_argnums=(2,3))\n","\n","\n","# =========================================================\n","# 5. Compute trajectory returns G (vectorized) and baseline\n","# =========================================================\n","@jit\n","def compute_returns(r_ts, x_T):\n","    \"\"\"\n","    r_ts: [H,B]\n","    x_T:  [B,D]\n","    return G: [B]\n","    G = sum_t r_t + log π̄(x_T)\n","    \"\"\"\n","    flow_term = jnp.sum(r_ts, axis=0)        # [B]\n","    term_bonus = batch_log_unnorm_pi(x_T)    # [B]\n","    return flow_term + term_bonus           # [B]\n","\n","\n","# =========================================================\n","# 6. Policy gradient loss (REINFORCE with baseline)\n","#\n","# We do ONE rollout, freeze those samples (xs, ts, acts, etc.),\n","# compute advantage = (G - baseline_mean),\n","# then build the loss:\n","#\n","#   L(θ) = - mean_b[ stop_grad(adv_b) * sum_t log πθ(a_t^b | x_t^b, t_t^b) ]\n","#\n","# where b indexes trajectories in the batch.\n","#\n","# That is unbiased REINFORCE.\n","# =========================================================\n","def reinforce_loss(actor_params,\n","                   xs, ts, acts,\n","                   adv,\n","                   sigma):\n","    \"\"\"\n","    xs:   [H,B,D]\n","    ts:   [H,B]\n","    acts: [H,B,D]\n","    adv:  [B]  (already stop_gradient outside)\n","    \"\"\"\n","    # We'll vm ap over t dimension to get logπθ at each t for each traj under *current* params.\n","    def per_t(x_t, t_t, a_t):\n","        return log_policy_gaussian(actor_params, x_t, t_t, a_t, sigma)  # [B]\n","\n","    logp_all = jax.vmap(per_t, in_axes=(0,0,0))(xs, ts, acts)  # [H,B]\n","    logp_sum = jnp.sum(logp_all, axis=0)                      # [B]\n","\n","    # REINFORCE objective:\n","    # J_hat = mean( adv * logp_sum )\n","    J_hat = jnp.mean(adv * logp_sum)\n","    return -J_hat  # we minimize loss\n","\n","reinforce_loss_jit = jit(reinforce_loss, static_argnums=(5,))\n","\n","\n","# =========================================================\n","# 7. Predictive metrics (jit)\n","# =========================================================\n","@jit\n","def predictive_metrics(params_batch, X, y):\n","    # params_batch: [S,D]\n","    w = params_batch[:, :-1]         # [S,30]\n","    b = params_batch[:, -1:]         # [S,1]\n","    logits = X @ w.T + b.T           # [N,S]\n","    probs  = jax.nn.sigmoid(logits)  # [N,S]\n","    p_mc   = jnp.mean(probs, axis=1) # [N]\n","    eps = 1e-7\n","    nll = -jnp.mean(y*jnp.log(p_mc+eps) + (1-y)*jnp.log(1-p_mc+eps))\n","    acc = jnp.mean((p_mc >= 0.5) == (y >= 0.5))\n","    return nll, acc\n","\n","# helper to sample final params x_T for eval\n","def sample_final_params(key, actor_params, num_samples, sigma):\n","    key, traj = rollout_batch_once_jit(key, actor_params, num_samples, sigma)\n","    x_T = traj[\"x_T\"]  # [num_samples, D]\n","    return key, x_T\n","\n","\n","# =========================================================\n","# 8. Agent with JIT'd train_step\n","# =========================================================\n","class ReinforceAgent:\n","    def __init__(self,\n","                 sigma=SIGMA,\n","                 horizon=HORIZON,\n","                 lr=1e-4,\n","                 seed=0):\n","\n","        assert horizon == HORIZON, \"keep horizon global consistent for jit shapes\"\n","\n","        key = random.PRNGKey(seed)\n","        key, ka = random.split(key)\n","\n","        dummy_x = jnp.zeros((1, D))\n","        dummy_t = jnp.zeros((1,))\n","        actor = Actor(out_dim=D)\n","        actor_params = actor.init(ka, dummy_x, dummy_t)\n","\n","        self.actor = actor\n","        self.actor_state = TrainState.create(\n","            apply_fn=self.actor.apply,\n","            params=actor_params,\n","            tx=optax.adam(lr)\n","        )\n","\n","        # running return baseline (EMA)\n","        self.baseline_mean = 0.0\n","        self.baseline_beta = 0.9\n","\n","        self.key = key\n","        self.sigma = float(sigma)\n","\n","    @staticmethod\n","    @jit\n","    def _ema_update(old_mean, new_batch_mean, beta):\n","        return beta*old_mean + (1.0-beta)*new_batch_mean\n","\n","    def train_step(self, batch_size):\n","        \"\"\"\n","        1. rollout trajectories (JIT)\n","        2. compute returns and baseline/advantage\n","        3. JIT'd grad on reinforce_loss\n","        4. update actor params\n","        \"\"\"\n","        # rollout\n","        self.key, traj = rollout_batch_once_jit(\n","            self.key,\n","            self.actor_state.params,\n","            batch_size,\n","            self.sigma\n","        )\n","        xs     = traj[\"xs\"]        # [H,B,D]\n","        ts     = traj[\"ts\"]        # [H,B]\n","        acts   = traj[\"acts\"]      # [H,B,D]\n","        r_ts   = traj[\"r_ts\"]      # [H,B]\n","        x_T    = traj[\"x_T\"]       # [B,D]\n","\n","        # returns G per traj\n","        G = compute_returns(r_ts, x_T)  # [B]\n","\n","        # update EMA baseline on host (cheap)\n","        G_np = np.array(G)\n","        batch_mean = float(G_np.mean())\n","        self.baseline_mean = ReinforceAgent._ema_update(\n","            self.baseline_mean,\n","            batch_mean,\n","            self.baseline_beta\n","        )\n","\n","        # advantage (stop_gradient when fed to loss)\n","        adv = G - self.baseline_mean  # [B]\n","\n","        # build loss+grad function\n","        def loss_fn(params, xs, ts, acts, adv):\n","            # stop grad on adv here so loss doesn't backprop into baseline/returns\n","            adv_sg = jax.lax.stop_gradient(adv)\n","            return reinforce_loss_jit(params, xs, ts, acts, adv_sg, self.sigma)\n","\n","        loss_val, grads = value_and_grad(loss_fn)(\n","            self.actor_state.params, xs, ts, acts, adv\n","        )\n","\n","        # apply update\n","        self.actor_state = self.actor_state.apply_gradients(grads=grads)\n","\n","        avg_return = float(batch_mean)\n","        return float(loss_val), avg_return\n","\n","\n","# =========================================================\n","# 9. Training loop\n","# =========================================================\n","if __name__ == \"__main__\":\n","    LR = 1e-4\n","    BATCH_ROLLOUT = 1028\n","    TRAIN_ITERS = 4000\n","    EVAL_SAMPLES = 4000\n","\n","    agent = ReinforceAgent(\n","        sigma=SIGMA,\n","        horizon=HORIZON,\n","        lr=LR,\n","        seed=0\n","    )\n","\n","    print(\"Training REINFORCE-RLFS (JIT)...\")\n","    for it in trange(TRAIN_ITERS):\n","        loss, avg_ret = agent.train_step(BATCH_ROLLOUT)\n","\n","        if (it + 1) % 100 == 0:\n","            agent.key, params_T = sample_final_params(\n","                agent.key,\n","                agent.actor_state.params,\n","                EVAL_SAMPLES,\n","                agent.sigma\n","            )\n","            nll_tr, acc_tr = predictive_metrics(params_T, X_train, y_train)\n","            nll_te, acc_te = predictive_metrics(params_T, X_test,  y_test)\n","            print(f\"[Iter {it+1:04d}] \"\n","                  f\"Loss={loss:.3f} Ret={avg_ret:.2f} | \"\n","                  f\"Train NLL={float(nll_tr):.3f} Acc={float(acc_tr):.3f} | \"\n","                  f\"Test  NLL={float(nll_te):.3f} Acc={float(acc_te):.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uUV3d92KIZmD","executionInfo":{"status":"ok","timestamp":1761794675668,"user_tz":-480,"elapsed":146193,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"2f8d3f6b-9284-4f57-84f2-bf456c4dbb2f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Devices: [CudaDevice(id=0)]\n","Dataset: train=426, test=143, dim=30\n","Training REINFORCE-RLFS (JIT)...\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 103/4000 [00:10<09:24,  6.90it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0100] Loss=871732.938 Ret=-88550.21 | Train NLL=0.467 Acc=0.803 | Test  NLL=0.587 Acc=0.811\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 203/4000 [00:14<02:11, 28.88it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0200] Loss=846512.438 Ret=-62594.16 | Train NLL=0.452 Acc=0.805 | Test  NLL=0.506 Acc=0.804\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 303/4000 [00:18<02:10, 28.28it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0300] Loss=1271096.875 Ret=-37128.16 | Train NLL=0.358 Acc=0.908 | Test  NLL=0.395 Acc=0.846\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 404/4000 [00:21<02:03, 29.20it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0400] Loss=910320.562 Ret=-18421.64 | Train NLL=0.389 Acc=0.838 | Test  NLL=0.434 Acc=0.804\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 503/4000 [00:24<01:59, 29.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0500] Loss=289034.750 Ret=-8959.00 | Train NLL=0.355 Acc=0.852 | Test  NLL=0.396 Acc=0.818\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 602/4000 [00:28<02:47, 20.23it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0600] Loss=78360.203 Ret=-5232.82 | Train NLL=0.337 Acc=0.866 | Test  NLL=0.366 Acc=0.853\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 705/4000 [00:32<01:55, 28.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0700] Loss=46400.375 Ret=-3713.71 | Train NLL=0.316 Acc=0.873 | Test  NLL=0.343 Acc=0.860\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 804/4000 [00:35<01:48, 29.35it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0800] Loss=19125.041 Ret=-2924.85 | Train NLL=0.249 Acc=0.908 | Test  NLL=0.280 Acc=0.888\n"]},{"output_type":"stream","name":"stderr","text":[" 23%|██▎       | 903/4000 [00:38<01:45, 29.43it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0900] Loss=31599.031 Ret=-2411.22 | Train NLL=0.243 Acc=0.918 | Test  NLL=0.270 Acc=0.902\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 1003/4000 [00:42<01:42, 29.24it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1000] Loss=-8747.858 Ret=-2217.12 | Train NLL=0.210 Acc=0.930 | Test  NLL=0.240 Acc=0.923\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 1106/4000 [00:45<01:37, 29.62it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1100] Loss=4807.268 Ret=-1985.79 | Train NLL=0.191 Acc=0.941 | Test  NLL=0.221 Acc=0.937\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 1206/4000 [00:49<01:33, 29.85it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1200] Loss=15746.478 Ret=-1806.21 | Train NLL=0.180 Acc=0.941 | Test  NLL=0.214 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 1304/4000 [00:52<02:09, 20.84it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1300] Loss=5534.780 Ret=-1690.58 | Train NLL=0.165 Acc=0.946 | Test  NLL=0.195 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 1404/4000 [00:56<01:27, 29.72it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1400] Loss=13286.316 Ret=-1556.15 | Train NLL=0.161 Acc=0.948 | Test  NLL=0.192 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 1503/4000 [00:59<01:25, 29.32it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1500] Loss=3734.760 Ret=-1493.38 | Train NLL=0.160 Acc=0.948 | Test  NLL=0.188 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 1606/4000 [01:02<01:20, 29.60it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1600] Loss=3248.963 Ret=-1405.01 | Train NLL=0.148 Acc=0.951 | Test  NLL=0.178 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 1703/4000 [01:06<01:19, 28.91it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1700] Loss=7516.651 Ret=-1331.88 | Train NLL=0.151 Acc=0.953 | Test  NLL=0.177 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▌     | 1803/4000 [01:09<01:15, 29.12it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1800] Loss=4719.381 Ret=-1273.39 | Train NLL=0.140 Acc=0.951 | Test  NLL=0.172 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 1906/4000 [01:13<01:09, 30.07it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1900] Loss=136.600 Ret=-1221.53 | Train NLL=0.141 Acc=0.946 | Test  NLL=0.172 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 2002/4000 [01:16<01:36, 20.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2000] Loss=-5901.017 Ret=-1181.70 | Train NLL=0.134 Acc=0.953 | Test  NLL=0.166 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 53%|█████▎    | 2105/4000 [01:20<01:02, 30.13it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2100] Loss=8013.326 Ret=-1101.36 | Train NLL=0.131 Acc=0.951 | Test  NLL=0.162 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 2205/4000 [01:23<00:59, 30.17it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2200] Loss=-4717.550 Ret=-1085.97 | Train NLL=0.130 Acc=0.948 | Test  NLL=0.162 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 2304/4000 [01:26<00:57, 29.50it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2300] Loss=3209.830 Ret=-1028.58 | Train NLL=0.127 Acc=0.958 | Test  NLL=0.158 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 2405/4000 [01:30<00:53, 29.71it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2400] Loss=1607.092 Ret=-991.41 | Train NLL=0.124 Acc=0.955 | Test  NLL=0.155 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 63%|██████▎   | 2504/4000 [01:34<00:51, 29.01it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2500] Loss=30.452 Ret=-961.06 | Train NLL=0.124 Acc=0.948 | Test  NLL=0.159 Acc=0.944\n"]},{"output_type":"stream","name":"stderr","text":[" 65%|██████▌   | 2603/4000 [01:37<00:49, 28.38it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2600] Loss=-314.952 Ret=-927.74 | Train NLL=0.125 Acc=0.958 | Test  NLL=0.157 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 2702/4000 [01:41<01:03, 20.34it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2700] Loss=1804.136 Ret=-888.07 | Train NLL=0.121 Acc=0.962 | Test  NLL=0.153 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 2803/4000 [01:44<00:41, 28.83it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2800] Loss=2708.704 Ret=-857.13 | Train NLL=0.122 Acc=0.955 | Test  NLL=0.154 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 2905/4000 [01:47<00:37, 29.04it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2900] Loss=45.579 Ret=-834.99 | Train NLL=0.122 Acc=0.953 | Test  NLL=0.153 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 3005/4000 [01:51<00:33, 29.52it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3000] Loss=3907.437 Ret=-802.77 | Train NLL=0.118 Acc=0.962 | Test  NLL=0.147 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 3104/4000 [01:55<00:30, 29.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3100] Loss=4669.167 Ret=-772.93 | Train NLL=0.116 Acc=0.967 | Test  NLL=0.145 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 3203/4000 [01:58<00:27, 29.17it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3200] Loss=4724.407 Ret=-750.23 | Train NLL=0.117 Acc=0.962 | Test  NLL=0.146 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 83%|████████▎ | 3303/4000 [02:01<00:23, 29.43it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3300] Loss=296.412 Ret=-731.25 | Train NLL=0.113 Acc=0.960 | Test  NLL=0.142 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 3403/4000 [02:05<00:28, 20.59it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3400] Loss=-2436.727 Ret=-718.32 | Train NLL=0.110 Acc=0.967 | Test  NLL=0.141 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 3504/4000 [02:08<00:16, 29.74it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3500] Loss=834.009 Ret=-689.08 | Train NLL=0.109 Acc=0.969 | Test  NLL=0.139 Acc=0.958\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 3603/4000 [02:12<00:13, 29.59it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3600] Loss=3689.060 Ret=-662.80 | Train NLL=0.108 Acc=0.967 | Test  NLL=0.138 Acc=0.958\n"]},{"output_type":"stream","name":"stderr","text":[" 93%|█████████▎| 3706/4000 [02:15<00:09, 29.58it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3700] Loss=-2962.834 Ret=-657.62 | Train NLL=0.107 Acc=0.967 | Test  NLL=0.137 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▌| 3806/4000 [02:19<00:06, 29.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3800] Loss=-3204.368 Ret=-638.04 | Train NLL=0.108 Acc=0.967 | Test  NLL=0.139 Acc=0.958\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 3906/4000 [02:22<00:03, 29.70it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3900] Loss=-439.293 Ret=-613.80 | Train NLL=0.106 Acc=0.965 | Test  NLL=0.135 Acc=0.958\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4000/4000 [02:25<00:00, 27.46it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4000] Loss=-576.525 Ret=-597.76 | Train NLL=0.104 Acc=0.969 | Test  NLL=0.134 Acc=0.951\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# DPG"],"metadata":{"id":"f8i5fQ31dcFd"}},{"cell_type":"code","source":["!pip install numpyro"],"metadata":{"id":"-_Zmqlk2CMx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import time\n","import numpy as np\n","import jax, jax.numpy as jnp\n","import numpyro\n","import numpyro.distributions as dist\n","from numpyro.infer import MCMC, NUTS\n","from scipy.special import expit as sigmoid\n","from scipy.stats import ks_2samp\n","import arviz as az\n","from scipy.stats import wasserstein_distance\n","from sklearn.metrics.pairwise import pairwise_kernels\n","\n","def make_blr_data():\n","    data = load_breast_cancer()\n","    X = data.data.astype(np.float32)\n","    y = data.target.astype(np.int32)\n","    X = StandardScaler().fit_transform(X)\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.25, random_state=0\n","    )\n","    return jnp.array(X_train), jnp.array(y_train), jnp.array(X_test), jnp.array(y_test)\n","\n","X_train, y_train, X_test, y_test = make_blr_data()\n","D_base = int(X_train.shape[1])\n","D = D_base + 1\n","\n","# ======================================\n","#  Define Bayesian Logistic Regression model\n","# ======================================\n","TAU = 2.5  # prior stddev\n","\n","def blr_model(X, y=None):\n","    D = X.shape[1]\n","    w = numpyro.sample(\"w\", dist.Normal(0.0, TAU).expand([D]))\n","    b = numpyro.sample(\"b\", dist.Normal(0.0, TAU))\n","    logits = jnp.dot(X, w) + b\n","    numpyro.sample(\"y\", dist.Bernoulli(logits=logits), obs=y)\n","\n","# ======================================\n","# Run NUTS (Hamiltonian Monte Carlo)\n","# ======================================\n","nuts = NUTS(blr_model, target_accept_prob=0.9, dense_mass=True)\n","mcmc = MCMC(nuts, num_warmup=1000, num_samples=1000, num_chains=4, chain_method=\"parallel\")\n","\n","rng_key = jax.random.PRNGKey(0)\n","print(\"🚀 Running NUTS sampling ...\")\n","t0 = time.time()\n","mcmc.run(rng_key, X=X_train, y=y_train)\n","wall = time.time() - t0\n","print(f\"⏱ Done in {wall:.1f} sec\")\n","\n","# ======================================\n","# Diagnostics with ArviZ\n","# ======================================\n","idata = az.from_numpyro(mcmc)\n","diverging = np.array(mcmc.get_extra_fields()[\"diverging\"]).sum()\n","print(f\"⚠️ Divergences: {diverging}\")\n","\n","# ======================================\n","# Posterior predictive check (test log-likelihood)\n","# ======================================\n","samples = mcmc.get_samples(group_by_chain=False)\n","\n","def test_log_pred_density(samples_flat, Xs, ys, batch=512):\n","    W = np.array(samples_flat[\"w\"])  # [S, D]\n","    b = np.array(samples_flat[\"b\"])  # [S]\n","    S = W.shape[0]\n","    total = 0.0\n","    for start in range(0, Xs.shape[0], batch):\n","        end = min(start + batch, Xs.shape[0])\n","        Xb = np.array(Xs[start:end])\n","        yb = np.array(ys[start:end])[:, None]\n","        logits = Xb @ W.T + b  # [B, S]\n","        p = 1 / (1 + np.exp(-logits))\n","        per_samp = p**yb * (1 - p)**(1 - yb)\n","        avg = np.clip(per_samp.mean(axis=1), 1e-12, 1.0)\n","        total += np.log(avg).sum()\n","    return total / Xs.shape[0]\n","\n","tll = test_log_pred_density(samples, X_test, y_test)\n","print(f\"📊 Test log predictive density (nats/sample): {tll:.4f}\")\n","\n","# ======================================\n","#  Save reference posterior\n","# ======================================\n","np.savez_compressed(\n","    \"blr_nuts_reference.npz\",\n","    w=np.array(samples[\"w\"]),\n","    b=np.array(samples[\"b\"]),\n","    tll=tll,\n","    wall=wall\n",")\n","print(\"💾 Saved posterior to blr_nuts_reference.npz\")\n","\n","# ======================================\n","# Comparison helper\n","# ======================================\n","def compare_to_reference(samples_other, samples_ref, max_points=2000):\n","    \"\"\"Compare RLFS samples to NUTS reference using Wasserstein & MMD (compact).\"\"\"\n","    W_oth, W_ref = np.array(samples_other[\"w\"]), np.array(samples_ref[\"w\"])\n","    B_oth, B_ref = np.array(samples_other[\"b\"]), np.array(samples_ref[\"b\"])\n","    if W_oth.shape[0] > max_points:\n","        W_oth = W_oth[np.random.choice(W_oth.shape[0], max_points, replace=False)]\n","    if W_ref.shape[0] > max_points:\n","        W_ref = W_ref[np.random.choice(W_ref.shape[0], max_points, replace=False)]\n","\n","    D = W_ref.shape[1]\n","    ws = [wasserstein_distance(W_oth[:, d], W_ref[:, d]) for d in range(D)]\n","    w_mean, w_max, b_ws = np.mean(ws), np.max(ws), wasserstein_distance(B_oth, B_ref)\n","\n","    X = np.vstack([W_ref, W_oth])\n","    dists = np.sum((X[:, None, :] - X[None, :, :])**2, axis=-1)\n","    gamma = 1.0 / (2 * np.median(dists[dists > 0]) + 1e-12)\n","    def mmd(X, Y):\n","        Kxx = np.mean(np.exp(-gamma * np.sum((X[:, None]-X[None])**2, axis=-1)))\n","        Kyy = np.mean(np.exp(-gamma * np.sum((Y[:, None]-Y[None])**2, axis=-1)))\n","        Kxy = np.mean(np.exp(-gamma * np.sum((X[:, None]-Y[None])**2, axis=-1)))\n","        return Kxx + Kyy - 2*Kxy\n","    mmd_val = mmd(W_ref, W_oth)\n","\n","    mu_err = np.mean(np.abs(W_oth.mean(0) - W_ref.mean(0)))\n","    var_err = np.mean(np.abs(W_oth.std(0) - W_ref.std(0)))\n","\n","    return dict(w_mean=w_mean, w_max=w_max, b_ws=b_ws, mmd=mmd_val,\n","                mu_err=mu_err, var_err=var_err)\n","\n","print(\"✅ Reference posterior ready — use compare_to_reference() for your RLFS samples.\")\n","\n","import numpy as np\n","\n","ref = np.load(\"blr_nuts_reference.npz\")\n","samples_ref = dict(w=ref[\"w\"], b=ref[\"b\"])\n","print(f\"NUTS samples: w={samples_ref['w'].shape}, b={samples_ref['b'].shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EW5Ez-dQ9qlb","executionInfo":{"status":"ok","timestamp":1761720428652,"user_tz":-480,"elapsed":120512,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}},"outputId":"23c90374-c584-4265-b1dd-2cfb0944e3cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3082415198.py:34: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n","  mcmc = MCMC(nuts, num_warmup=1000, num_samples=1000, num_chains=4, chain_method=\"parallel\")\n"]},{"output_type":"stream","name":"stdout","text":["🚀 Running NUTS sampling ...\n"]},{"output_type":"stream","name":"stderr","text":["sample: 100%|██████████| 2000/2000 [00:28<00:00, 71.30it/s, 15 steps of size 2.71e-01. acc. prob=0.93] \n","sample: 100%|██████████| 2000/2000 [00:23<00:00, 86.27it/s, 15 steps of size 2.66e-01. acc. prob=0.93] \n","sample: 100%|██████████| 2000/2000 [00:22<00:00, 89.67it/s, 15 steps of size 2.63e-01. acc. prob=0.94] \n","sample: 100%|██████████| 2000/2000 [00:24<00:00, 80.67it/s, 15 steps of size 3.59e-01. acc. prob=0.86] \n"]},{"output_type":"stream","name":"stdout","text":["⏱ Done in 109.1 sec\n","⚠️ Divergences: 0\n","📊 Test log predictive density (nats/sample): -0.0997\n","💾 Saved posterior to blr_nuts_reference.npz\n","✅ Reference posterior ready — use compare_to_reference() for your RLFS samples.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3082415198.py:65: RuntimeWarning: overflow encountered in exp\n","  p = 1 / (1 + np.exp(-logits))\n"]}]},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","from jax import random, lax, jit, grad, device_put\n","from flax import linen as nn\n","from flax.training.train_state import TrainState\n","import optax\n","import numpy as np\n","from tqdm import trange\n","import time\n","\n","print(\"Devices:\", jax.devices())\n","\n","# ================================================================\n","# Bayesian Logistic Regression target\n","# ===============================================================\n","\n","def blr_log_unnormalized(params, X, y, alpha=1.0):\n","    \"\"\"\n","    Unnormalized log posterior:\n","        log π̄(w,b) = log p(y|X,w,b) + log p(w,b)\n","    where p(w,b) = N(0, α⁻¹ I)\n","    \"\"\"\n","    w, b = params[:-1], params[-1]\n","\n","    logits = X @ w + b\n","    log_lik = jnp.sum(\n","        y * jax.nn.log_sigmoid(logits) + (1 - y) * jax.nn.log_sigmoid(-logits)\n","    )\n","    log_prior = -0.5 * alpha * jnp.dot(params, params)\n","    return log_lik + log_prior\n","\n","# ================================================================\n","# Models\n","# ================================================================\n","class TimeEmbed(nn.Module):\n","    hidden: int = 64\n","    @nn.compact\n","    def __call__(self, t):\n","        freqs = jnp.asarray([1., 2., 4., 8., 16.])\n","        sinus = jnp.concatenate([\n","            jnp.sin(2*jnp.pi*freqs[None,:] * t[:,None]),\n","            jnp.cos(2*jnp.pi*freqs[None,:] * t[:,None])\n","        ], axis=-1)\n","        h = nn.relu(nn.Dense(self.hidden)(sinus))\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        return h\n","\n","class Actor(nn.Module):\n","    hidden: int = 128\n","    out_dim: int = 2\n","    act_scale: float = 2.0\n","    @nn.compact\n","    def __call__(self, x, t):\n","        te = TimeEmbed()(t)\n","        h = jnp.concatenate([x, te], axis=-1)\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        h = nn.LayerNorm()(h)\n","        a = nn.Dense(self.out_dim)(h)\n","        return self.act_scale * jnp.tanh(a)\n","\n","class CriticQ(nn.Module):\n","    hidden: int = 256\n","    @nn.compact\n","    def __call__(self, x, a, t):\n","        te = TimeEmbed()(t)\n","        h = jnp.concatenate([x, a, te], axis=-1)\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        h = nn.relu(nn.Dense(self.hidden)(h))\n","        h = nn.LayerNorm()(h)\n","        return nn.Dense(1)(h).squeeze(-1)\n","\n","def make_actor_forward(D: int):\n","    module = Actor(out_dim=D)        # built outside jit → static\n","    @jax.jit\n","    def _forward(params, x, t):\n","        return module.apply(params, x, t)\n","    return _forward\n","\n","@jit\n","def critic_forward(params, x, a, t):\n","    return CriticQ().apply(params, x, a, t)\n","\n","def critic_grads_factory(actor_forward):\n","    @jax.jit\n","    def _critic_grads(critic_params, critic_targ, actor_targ, batch, gamma):\n","        x, a, t, r, xn, tn, done = batch\n","        def loss_fn(params):\n","            a_next = actor_forward(actor_targ, xn, tn)\n","            q_next = critic_forward(critic_targ, xn, a_next, tn)\n","            y = r + (1.0 - done) * gamma * q_next\n","            y = jax.lax.stop_gradient(y)\n","            q = critic_forward(params, x, a, t)\n","            return jnp.mean((q - y)**2)\n","        return jax.grad(loss_fn)(critic_params)\n","    return _critic_grads\n","\n","def actor_grads_factory(actor_forward):\n","    @jax.jit\n","    def _actor_grads(actor_params, critic_params, batch):\n","        x, _, t, _, _, _, _ = batch\n","        def loss_fn(params):\n","            a_pred = actor_forward(params, x, t)\n","            q = critic_forward(critic_params, x, a_pred, t)\n","            return -jnp.mean(q)\n","        return jax.grad(loss_fn)(actor_params)\n","    return _actor_grads\n","\n","# ================================================================\n","# RLFS “env” bits (pure JAX)\n","# ================================================================\n","@jit\n","def logF(x, x_next, a, sigma):\n","    diff = x_next - jnp.sqrt(1 - sigma**2) * x - a\n","    return -jnp.sum(diff**2, axis=-1) / (2*sigma**2)\n","\n","@jit\n","def logB(x, x_next, sigma):\n","    diff = x - jnp.sqrt(1 - sigma**2) * x_next\n","    return -jnp.sum(diff**2, axis=-1) / (2*sigma**2)\n","\n","# ---- Rollout factory that closes over T and D\n","def make_rollout_trajectory(T: int, actor_forward):\n","    invT = jnp.array(1.0 / T, dtype=jnp.float32)\n","\n","    def rollout_step(carry, _):\n","        key, x, t, actor_params, sigma = carry\n","        key, sub = random.split(key)\n","        a = actor_forward(actor_params, x, t)\n","        eps = random.normal(sub, x.shape)\n","        x_next = jnp.sqrt(1-sigma**2) *x + a + sigma * eps\n","        t_next = t + invT\n","        r_step = logB(x, x_next, sigma) - logF(x, x_next, a, sigma)\n","        carry_next = (key, x_next, t_next, actor_params, sigma)\n","        trans = (x, a, t, r_step, x_next, t_next)\n","        return carry_next, trans\n","\n","    @jit\n","    def rollout_trajectory(key, x0, t0, actor_params, sigma):\n","        init = (key, x0, t0, actor_params, sigma)\n","        (key_f, xT, tT, _, _), (xs, as_, ts, rs, xns, tns) = lax.scan(\n","            rollout_step, init, xs=None, length=T\n","        )\n","        return (xs, as_, ts, rs, xns, tns), (xT, tT)\n","\n","    return rollout_trajectory\n","\n","# ================================================================\n","# Replay Buffer\n","# ================================================================\n","class ReplayBuffer:\n","    def __init__(self, capacity, obs_dim, act_dim):\n","        self.capacity = int(capacity)\n","        self.ptr = 0\n","        self.size = 0\n","        self.x = np.zeros((capacity, obs_dim), np.float32)\n","        self.a = np.zeros((capacity, act_dim), np.float32)\n","        self.t = np.zeros((capacity,), np.float32)\n","        self.r = np.zeros((capacity,), np.float32)\n","        self.xn = np.zeros((capacity, obs_dim), np.float32)\n","        self.tn = np.zeros((capacity,), np.float32)\n","        self.done = np.zeros((capacity,), np.float32)\n","\n","    def push_batch(self, x, a, t, r, xn, tn, done):\n","        B = x.shape[0]\n","        idx = (np.arange(B) + self.ptr) % self.capacity\n","        self.x[idx] = x; self.a[idx] = a; self.t[idx] = t; self.r[idx] = r\n","        self.xn[idx] = xn; self.tn[idx] = tn; self.done[idx] = done\n","        self.ptr = (self.ptr + B) % self.capacity\n","        self.size = int(min(self.capacity, self.size + B))\n","\n","    def sample(self, batch_size):\n","        idx = np.random.randint(0, self.size, size=batch_size)\n","        return (self.x[idx], self.a[idx], self.t[idx], self.r[idx],\n","                self.xn[idx], self.tn[idx], self.done[idx])\n","\n","    def __len__(self):\n","        return self.size\n","\n","# ===================\n","# Agent (DDPG-style)\n","# ===================\n","class Agent:\n","    def __init__(self, sigma=0.9, T=64, lr_actor=3e-4, lr_critic=3e-4,\n","                 tau=0.01, gamma=1.0, seed=0, D=2):\n","        self.sigma, self.T, self.tau, self.gamma = float(sigma), int(T), float(tau), float(gamma)\n","        self.D = int(D)\n","        key = random.PRNGKey(seed)\n","        dummy_x = jnp.zeros((1, D)); dummy_a = jnp.zeros((1, D)); dummy_t = jnp.zeros((1,))\n","        key, ka, kc = random.split(key, 3)\n","        actor = Actor(out_dim=D)\n","        critic = CriticQ()\n","        actor_params = actor.init(ka, dummy_x, dummy_t)\n","        critic_params = critic.init(kc, dummy_x, dummy_a, dummy_t)\n","        self.actor_state = TrainState.create(apply_fn=actor.apply, params=actor_params, tx=optax.adam(lr_actor))\n","        self.critic_state = TrainState.create(apply_fn=critic.apply, params=critic_params, tx=optax.adam(lr_critic))\n","        self.actor_targ = self.actor_state.params\n","        self.critic_targ = self.critic_state.params\n","        self.key = key\n","        self.actor_forward = make_actor_forward(D)\n","        self._critic_grads = critic_grads_factory(self.actor_forward)\n","        self._actor_grads  = actor_grads_factory(self.actor_forward)\n","\n","\n","    @staticmethod\n","    @jit\n","    def soft_update(target, source, tau):\n","        return jax.tree_util.tree_map(lambda t, s: (1-tau)*t + tau*s, target, source)\n","\n","    def update(self, batch):\n","        batch_dev = tuple(device_put(jnp.asarray(b)) for b in batch)\n","        gC = self._critic_grads(self.critic_state.params, self.critic_targ, self.actor_targ,\n","                                batch_dev, self.gamma)\n","        self.critic_state = self.critic_state.apply_gradients(grads=gC)\n","        gA = self._actor_grads(self.actor_state.params, self.critic_state.params,\n","                               batch_dev)\n","        self.actor_state = self.actor_state.apply_gradients(grads=gA)\n","        self.actor_targ = self.soft_update(self.actor_targ, self.actor_state.params, self.tau)\n","        self.critic_targ = self.soft_update(self.critic_targ, self.critic_state.params, self.tau)\n","\n","# ================================================================\n","# Training\n","# ================================================================\n","if __name__ == '__main__':\n","    # Hyperparameters\n","    SIGMA = 0.9\n","    T_H = 50\n","    B_COLLECT = 4096\n","    REPLAY_CAP = 1_000_000\n","    START_STEPS = 65_536\n","    TRAIN_ITERS = 5000\n","    B_UPDATE = 2048\n","    UPDATES_PER_ITER = 2\n","    TAU = 0.01\n","    LR_ACTOR = 1e-5\n","    LR_CRITIC= 3e-5\n","    ALPHA = 1/2.5**2  # prior precision\n","\n","    # ================================================================\n","    # Bayesian Logistic Regression\n","    # ================================================================\n","\n","    # Agent, replay, rollout fn with static T and D\n","    agent = Agent(sigma=SIGMA, T=T_H, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, tau=TAU, D=D)\n","    rb = ReplayBuffer(REPLAY_CAP, obs_dim=D, act_dim=D)\n","    rollout_trajectory = make_rollout_trajectory(T_H, actor_forward=agent.actor_forward)\n","\n","    # Terminal reward for each trajectory end\n","    def batch_log_unnorm_pi(xT_batch):\n","        # vmap over the batch of terminal weights\n","        return jax.vmap(lambda params: blr_log_unnormalized(params, X_train, y_train, ALPHA))(xT_batch)\n","\n","    # Warmup\n","    print('Collecting warmup...')\n","    for _ in trange(max(1, START_STEPS // B_COLLECT)):\n","        agent.key, sub = random.split(agent.key)\n","        x0 = 2.5 * random.normal(sub, (B_COLLECT, D)); t0 = jnp.zeros((B_COLLECT,))\n","        (xs, as_, ts, rs, xns, tns), (xT, tT) = rollout_trajectory(agent.key, x0, t0, agent.actor_state.params, SIGMA)\n","        r_term = batch_log_unnorm_pi(xT)                        # shape (B,)\n","        # After computing r_term\n","        rs = rs.at[-1].set(rs[-1] + r_term)  # OK shape-wise\n","        # Set done = 1 only for the *last transition* of each trajectory\n","        DONE = np.zeros_like(TT)\n","        DONE[-B_COLLECT:] = 1.0\n","        X  = np.array(xs.reshape(-1, D))\n","        A  = np.array(as_.reshape(-1, D))\n","        TT = np.array(ts.reshape(-1))\n","        R  = np.array(rs.reshape(-1))\n","        XN = np.array(xns.reshape(-1, D))\n","        TN = np.array(tns.reshape(-1))\n","        rb.push_batch(X, A, TT, R, XN, TN, DONE)\n","\n","    # Train\n","    print('Training...')\n","    t0_wall = time.time()\n","    for it in trange(TRAIN_ITERS):\n","        agent.key, sub = random.split(agent.key)\n","        x0 = 2.5 * random.normal(sub, (B_COLLECT, D)); t0 = jnp.zeros((B_COLLECT,))\n","        (xs, as_, ts, rs, xns, tns), (xT, tT) = rollout_trajectory(agent.key, x0, t0, agent.actor_state.params, SIGMA)\n","        r_term = batch_log_unnorm_pi(xT)\n","        rs = rs.at[-1].add(r_term)\n","\n","        X  = np.array(xs.reshape(-1, D))\n","        A  = np.array(as_.reshape(-1, D))\n","        TT = np.array(ts.reshape(-1))\n","        R  = np.array(rs.reshape(-1))\n","        XN = np.array(xns.reshape(-1, D))\n","        TN = np.array(tns.reshape(-1))\n","        DONE = np.zeros_like(TT); DONE[-B_COLLECT:] = 1.0\n","        rb.push_batch(X, A, TT, R, XN, TN, DONE)\n","\n","        if len(rb) >= START_STEPS:\n","            for _ in range(UPDATES_PER_ITER):\n","                batch = rb.sample(B_UPDATE)\n","                agent.update(batch)\n","\n","        if (it + 1) % 50 == 0:\n","            agent.key, sub = random.split(agent.key)\n","            x0 = 2.5 * random.normal(sub, (8000, D)); t0 = jnp.zeros((8000,))\n","            (_, _, _, _, _, _), (wT, tT) = rollout_trajectory(agent.key, x0, t0, agent.actor_state.params, agent.sigma)\n","            w_only = wT[:, :D_base]   # shape (8000, D_base)\n","            b_only = wT[:, -1]        # shape (8000,)\n","            samples_rlfs = dict(w=w_only, b=b_only)\n","\n","            metrics = compare_to_reference(samples_rlfs, samples_ref)\n","            print(f\"[Iter {it+1:04d}] \"\n","                  f\"Wmean={metrics['w_mean']:.3f} \"\n","                  f\"Wmax={metrics['w_max']:.3f} \"\n","                  f\"B={metrics['b_ws']:.3f} \"\n","                  f\"MMD={metrics['mmd']:.4f} \"\n","                  f\"MeanErr={metrics['mu_err']:.3f} \"\n","                  f\"VarErr={metrics['var_err']:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fTtG8O-2UlR7","outputId":"beb2ba77-2c70-4ce6-fdca-2e1b2dac0c49","executionInfo":{"status":"error","timestamp":1761722719475,"user_tz":-480,"elapsed":622416,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Devices: [CudaDevice(id=0)]\n","Collecting warmup...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:02<00:00,  6.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training...\n"]},{"output_type":"stream","name":"stderr","text":["  1%|          | 51/5000 [00:16<1:24:33,  1.03s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0050] Wmean=2.347 Wmax=5.166 B=0.637 MMD=0.5417 MeanErr=2.277 VarErr=0.490\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 100/5000 [00:29<1:31:09,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0100] Wmean=2.008 Wmax=4.882 B=0.837 MMD=0.4310 MeanErr=1.967 VarErr=0.411\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 150/5000 [00:42<1:33:38,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0150] Wmean=1.980 Wmax=5.163 B=1.424 MMD=0.4491 MeanErr=1.935 VarErr=0.401\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 201/5000 [00:55<1:10:10,  1.14it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0200] Wmean=1.699 Wmax=5.298 B=0.818 MMD=0.3756 MeanErr=1.614 VarErr=0.412\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▌         | 251/5000 [01:08<1:09:59,  1.13it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0250] Wmean=1.713 Wmax=5.348 B=1.931 MMD=0.3353 MeanErr=1.637 VarErr=0.410\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 301/5000 [01:21<1:10:38,  1.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0300] Wmean=1.686 Wmax=4.793 B=1.646 MMD=0.3299 MeanErr=1.591 VarErr=0.422\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 351/5000 [01:34<1:09:40,  1.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0350] Wmean=1.755 Wmax=4.644 B=1.924 MMD=0.3306 MeanErr=1.652 VarErr=0.446\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 400/5000 [01:47<1:26:28,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0400] Wmean=1.755 Wmax=4.232 B=1.969 MMD=0.3136 MeanErr=1.649 VarErr=0.427\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 451/5000 [02:00<1:00:47,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0450] Wmean=1.735 Wmax=4.371 B=1.863 MMD=0.2991 MeanErr=1.647 VarErr=0.418\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 501/5000 [02:13<1:00:08,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0500] Wmean=1.722 Wmax=4.564 B=1.791 MMD=0.2934 MeanErr=1.630 VarErr=0.426\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 551/5000 [02:26<59:17,  1.25it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 0550] Wmean=1.703 Wmax=4.844 B=1.724 MMD=0.2889 MeanErr=1.610 VarErr=0.428\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 601/5000 [02:39<58:43,  1.25it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 0600] Wmean=1.690 Wmax=4.829 B=1.591 MMD=0.2851 MeanErr=1.606 VarErr=0.433\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 651/5000 [02:52<57:56,  1.25it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 0650] Wmean=1.679 Wmax=4.808 B=1.445 MMD=0.2829 MeanErr=1.609 VarErr=0.426\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 701/5000 [03:05<58:04,  1.23it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 0700] Wmean=1.653 Wmax=4.455 B=1.333 MMD=0.2719 MeanErr=1.575 VarErr=0.431\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 751/5000 [03:18<56:22,  1.26it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 0750] Wmean=1.642 Wmax=4.522 B=1.232 MMD=0.2712 MeanErr=1.569 VarErr=0.424\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 800/5000 [03:31<1:15:16,  1.08s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0800] Wmean=1.609 Wmax=4.112 B=1.180 MMD=0.2599 MeanErr=1.537 VarErr=0.419\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 850/5000 [03:44<1:18:23,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0850] Wmean=1.601 Wmax=3.838 B=1.111 MMD=0.2535 MeanErr=1.528 VarErr=0.432\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 901/5000 [03:57<1:00:43,  1.12it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0900] Wmean=1.592 Wmax=3.645 B=1.069 MMD=0.2523 MeanErr=1.523 VarErr=0.421\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▉        | 951/5000 [04:10<59:40,  1.13it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 0950] Wmean=1.573 Wmax=3.464 B=1.065 MMD=0.2470 MeanErr=1.507 VarErr=0.420\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 1001/5000 [04:23<58:35,  1.14it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1000] Wmean=1.567 Wmax=3.279 B=1.028 MMD=0.2447 MeanErr=1.495 VarErr=0.433\n"]},{"output_type":"stream","name":"stderr","text":[" 21%|██        | 1051/5000 [04:35<59:15,  1.11it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1050] Wmean=1.571 Wmax=3.177 B=1.023 MMD=0.2442 MeanErr=1.500 VarErr=0.419\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 1101/5000 [04:48<58:35,  1.11it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1100] Wmean=1.568 Wmax=3.173 B=1.036 MMD=0.2417 MeanErr=1.494 VarErr=0.435\n"]},{"output_type":"stream","name":"stderr","text":[" 23%|██▎       | 1151/5000 [05:01<53:42,  1.19it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1150] Wmean=1.560 Wmax=3.122 B=1.040 MMD=0.2382 MeanErr=1.488 VarErr=0.438\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 1201/5000 [05:14<51:26,  1.23it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1200] Wmean=1.553 Wmax=3.061 B=0.988 MMD=0.2382 MeanErr=1.485 VarErr=0.433\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 1251/5000 [05:28<50:28,  1.24it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1250] Wmean=1.575 Wmax=3.148 B=1.003 MMD=0.2388 MeanErr=1.507 VarErr=0.423\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 1301/5000 [05:41<49:34,  1.24it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1300] Wmean=1.584 Wmax=3.212 B=0.990 MMD=0.2428 MeanErr=1.523 VarErr=0.416\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 1351/5000 [05:54<49:16,  1.23it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1350] Wmean=1.607 Wmax=3.361 B=0.992 MMD=0.2469 MeanErr=1.550 VarErr=0.424\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 1401/5000 [06:07<47:59,  1.25it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1400] Wmean=1.588 Wmax=3.464 B=0.997 MMD=0.2437 MeanErr=1.528 VarErr=0.408\n"]},{"output_type":"stream","name":"stderr","text":[" 29%|██▉       | 1451/5000 [06:20<47:47,  1.24it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1450] Wmean=1.613 Wmax=3.405 B=1.005 MMD=0.2494 MeanErr=1.560 VarErr=0.417\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 1501/5000 [06:33<47:01,  1.24it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1500] Wmean=1.621 Wmax=3.557 B=1.034 MMD=0.2511 MeanErr=1.563 VarErr=0.407\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███       | 1550/5000 [06:46<1:03:09,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1550] Wmean=1.631 Wmax=3.559 B=1.027 MMD=0.2533 MeanErr=1.575 VarErr=0.404\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 1600/5000 [06:59<1:04:11,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1600] Wmean=1.618 Wmax=3.585 B=1.048 MMD=0.2512 MeanErr=1.560 VarErr=0.408\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 1650/5000 [07:12<1:05:11,  1.17s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1650] Wmean=1.627 Wmax=3.582 B=1.060 MMD=0.2532 MeanErr=1.564 VarErr=0.418\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 1701/5000 [07:24<49:00,  1.12it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1700] Wmean=1.628 Wmax=3.586 B=1.091 MMD=0.2538 MeanErr=1.571 VarErr=0.407\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 1751/5000 [07:37<47:51,  1.13it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1750] Wmean=1.621 Wmax=3.549 B=1.131 MMD=0.2529 MeanErr=1.564 VarErr=0.414\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 1801/5000 [07:50<48:12,  1.11it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1800] Wmean=1.628 Wmax=3.538 B=1.118 MMD=0.2544 MeanErr=1.578 VarErr=0.408\n"]},{"output_type":"stream","name":"stderr","text":[" 37%|███▋      | 1851/5000 [08:03<46:37,  1.13it/s]  "]},{"output_type":"stream","name":"stdout","text":["[Iter 1850] Wmean=1.624 Wmax=3.585 B=1.114 MMD=0.2538 MeanErr=1.571 VarErr=0.400\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 1901/5000 [08:16<43:11,  1.20it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1900] Wmean=1.643 Wmax=3.516 B=1.169 MMD=0.2593 MeanErr=1.586 VarErr=0.407\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 1950/5000 [08:29<54:32,  1.07s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1950] Wmean=1.639 Wmax=3.523 B=1.195 MMD=0.2593 MeanErr=1.584 VarErr=0.405\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 2001/5000 [08:43<41:00,  1.22it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2000] Wmean=1.635 Wmax=3.523 B=1.186 MMD=0.2576 MeanErr=1.583 VarErr=0.404\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 2051/5000 [08:56<39:46,  1.24it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2050] Wmean=1.628 Wmax=3.556 B=1.189 MMD=0.2558 MeanErr=1.577 VarErr=0.403\n"]},{"output_type":"stream","name":"stderr","text":[" 42%|████▏     | 2101/5000 [09:09<39:21,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2100] Wmean=1.620 Wmax=3.449 B=1.200 MMD=0.2545 MeanErr=1.569 VarErr=0.392\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 2151/5000 [09:22<38:33,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2150] Wmean=1.621 Wmax=3.435 B=1.199 MMD=0.2570 MeanErr=1.571 VarErr=0.404\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 2201/5000 [09:35<37:48,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2200] Wmean=1.640 Wmax=3.455 B=1.212 MMD=0.2597 MeanErr=1.590 VarErr=0.394\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▌     | 2250/5000 [09:48<50:24,  1.10s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2250] Wmean=1.643 Wmax=3.457 B=1.195 MMD=0.2572 MeanErr=1.594 VarErr=0.405\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 2300/5000 [10:02<52:19,  1.16s/it]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2300] Wmean=1.616 Wmax=3.624 B=1.151 MMD=0.2544 MeanErr=1.562 VarErr=0.406\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 2351/5000 [10:15<39:07,  1.13it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2350] Wmean=1.645 Wmax=3.604 B=1.158 MMD=0.2604 MeanErr=1.591 VarErr=0.406\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 2374/5000 [10:19<11:25,  3.83it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3345897716.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUPDATES_PER_ITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_UPDATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3345897716.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    212\u001b[0m         gC = self._critic_grads(self.critic_state.params, self.critic_targ, self.actor_targ,\n\u001b[1;32m    213\u001b[0m                                 batch_dev, self.gamma)\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         gA = self._actor_grads(self.actor_state.params, self.critic_state.params,\n\u001b[1;32m    216\u001b[0m                                batch_dev)\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/flax/training/train_state.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mparams_with_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     updates, new_opt_state = self.tx.update(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mgrads_with_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_with_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optax/transforms/_combining.py\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(updates, state, params, **extra_args)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optax/_src/base.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mGradientTransformationExtraArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_moment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0mnu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_moment_per_elem_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mcount_inc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_increment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py\u001b[0m in \u001b[0;36mtree_update_moment\u001b[0;34m(updates, moments, decay, order)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtree_update_moment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0;34m\"\"\"Compute the exponential moving average of the `order`-th moment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   return jax.tree.map(\n\u001b[0m\u001b[1;32m    356\u001b[0m       lambda g, t: (\n\u001b[1;32m    357\u001b[0m           \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/tree.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g, t)\u001b[0m\n\u001b[1;32m    355\u001b[0m   return jax.tree.map(\n\u001b[1;32m    356\u001b[0m       lambda g, t: (\n\u001b[0;32m--> 357\u001b[0;31m           \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m       ),\n\u001b[1;32m    359\u001b[0m       \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/numpy/ufunc_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"where argument of {self}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__static_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'call'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_vectorized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# TD3 Approach with no replay buffer, shorter horizon"],"metadata":{"id":"TTmJVv7FfigY"}},{"cell_type":"code","source":["    # ===================== TD3-RLFS on Breast Cancer (Bayesian Logistic Regression) =====================\n","    # Twin critics, target policy smoothing, delayed policy updates, soft target updates, (optional) reward norm.\n","    # Objective unchanged: sum_t [log B - log F] + log π(x_T) with π the BLR unnormalized posterior on training data.\n","    # ---------------------------------------------------------------------------------------\n","    import jax\n","    import jax.numpy as jnp\n","    from jax import random, jit, lax, value_and_grad, device_put\n","    from flax import linen as nn\n","    from flax.training.train_state import TrainState\n","    import optax\n","    import numpy as np\n","    from tqdm import trange\n","\n","    from sklearn.datasets import load_breast_cancer\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.preprocessing import StandardScaler\n","\n","    print(\"Devices:\", jax.devices())\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # Dataset: Breast Cancer Wisconsin Diagnostic (569 samples, 30 features)\n","    # ---------------------------------------------------------------------------------------------------\n","    data = load_breast_cancer()\n","    X = data.data.astype(np.float32)\n","    y = data.target.astype(np.float32)\n","\n","    # Standardize features\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    # Train/Test split\n","    X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n","    X_train, X_test = jnp.array(X_train_np), jnp.array(X_test_np)\n","    y_train, y_test = jnp.array(y_train_np), jnp.array(y_test_np)\n","\n","    D_base = X_train.shape[1]  # 30\n","    D = D_base + 1             # weights + bias = 31\n","    print(f\"Dataset: train={X_train.shape[0]}, test={X_test.shape[0]}, dim={D_base}\")\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # BLR target (weights + bias concatenated)\n","    # ---------------------------------------------------------------------------------------------------\n","    def blr_log_unnormalized(params, X, y, alpha=1.0):\n","        \"\"\" log π̄(w,b) = log p(y|X,w,b) + log p(w,b), with Gaussian prior N(0, α⁻¹ I) \"\"\"\n","        w, b = params[:-1], params[-1]\n","        logits = X @ w + b\n","        log_lik = jnp.sum(y * jax.nn.log_sigmoid(logits) + (1 - y) * jax.nn.log_sigmoid(-logits))\n","        log_prior = -0.5 * alpha * jnp.dot(params, params)\n","        return log_lik + log_prior\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # Nets (TimeEmbed, Actor, Critic)\n","    # ---------------------------------------------------------------------------------------------------\n","    class TimeEmbed(nn.Module):\n","        hidden: int = 64\n","        @nn.compact\n","        def __call__(self, t):\n","            freqs = jnp.asarray([1., 2., 4., 8., 16.])\n","            sinus = jnp.concatenate([\n","                jnp.sin(2*jnp.pi*freqs[None,:] * t[:,None]),\n","                jnp.cos(2*jnp.pi*freqs[None,:] * t[:,None])\n","            ], axis=-1)\n","            h = nn.relu(nn.Dense(self.hidden)(sinus))\n","            h = nn.relu(nn.Dense(self.hidden)(h))\n","            return h\n","\n","    class Actor(nn.Module):\n","        hidden: int = 256\n","        out_dim: int = D     # 31\n","        act_scale: float = 3.0\n","        @nn.compact\n","        def __call__(self, x, t):\n","            te = TimeEmbed()(t)\n","            h = jnp.concatenate([x, te], axis=-1)\n","            h = nn.relu(nn.Dense(self.hidden)(h))\n","            h = nn.relu(nn.Dense(self.hidden)(h))\n","            a = nn.Dense(self.out_dim)(h)\n","            return self.act_scale * jnp.tanh(a)\n","\n","    class CriticQ(nn.Module):\n","        hidden: int = 256\n","        @nn.compact\n","        def __call__(self, x, a, t):\n","            te = TimeEmbed()(t)\n","            h = jnp.concatenate([x, a, te], axis=-1)\n","            h = nn.relu(nn.Dense(self.hidden)(h))\n","            h = nn.relu(nn.Dense(self.hidden)(h))\n","            return nn.Dense(1)(h).squeeze(-1)\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # RLFS kernels (ADDITIVE dynamics): x_{t+1} = sqrt(1-σ^2)*x_t + a + 0  (noise already added to a during rollout)\n","    # ---------------------------------------------------------------------------------------------------\n","    @jit\n","    def logF(x, x_next, a, sigma):\n","        muF  = x + a\n","        diff = x_next - muF\n","        return -jnp.sum(diff**2, axis=-1) / (2 * sigma**2)\n","\n","    @jit\n","    def logB(x, x_next, sigma):\n","        diff = x - jnp.sqrt(1 - sigma**2) * x_next\n","        return -jnp.sum(diff**2, axis=-1) / (2*sigma**2)\n","\n","    def make_rollout_trajectory(T, actor_forward):\n","        invT = jnp.array(1.0 / T, dtype=jnp.float32)\n","        def rollout_step(carry, _):\n","            key, x, t, actor_params, sigma = carry\n","            key, sub = random.split(key)\n","            a_det = actor_forward(actor_params, x, t)     # deterministic action\n","            eps   = random.normal(sub, x.shape)\n","            a     = a_det + sigma * eps                   # exploration noise\n","            x_next = x + a\n","            t_next = t + invT\n","            r_step = logB(x, x_next, sigma) - logF(x, x_next, a, sigma)\n","            carry_next = (key, x_next, t_next, actor_params, sigma)\n","            trans = (x, a_det, t, r_step, x_next, t_next)\n","            return carry_next, trans\n","\n","        @jit\n","        def rollout_trajectory(key, x0, t0, actor_params, sigma):\n","            init = (key, x0, t0, actor_params, sigma)\n","            (key_f, xT, tT, _, _), (xs, as_, ts, rs, xns, tns) = lax.scan(\n","                rollout_step, init, xs=None, length=T\n","            )\n","            return (xs, as_, ts, rs, xns, tns), (xT, tT)\n","        return rollout_trajectory\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # Replay Buffer\n","    # ---------------------------------------------------------------------------------------------------\n","    class ReplayBuffer:\n","        def __init__(self, capacity, obs_dim, act_dim):\n","            self.capacity = int(capacity); self.ptr = 0; self.size = 0\n","            self.x  = np.zeros((capacity, obs_dim), np.float32)\n","            self.a  = np.zeros((capacity, act_dim), np.float32)\n","            self.t  = np.zeros((capacity,), np.float32)\n","            self.r  = np.zeros((capacity,), np.float32)\n","            self.xn = np.zeros((capacity, obs_dim), np.float32)\n","            self.tn = np.zeros((capacity,), np.float32)\n","            self.done = np.zeros((capacity,), np.float32)\n","        def push_batch(self, x, a, t, r, xn, tn, done):\n","            B = x.shape[0]\n","            idx = (np.arange(B) + self.ptr) % self.capacity\n","            self.x[idx] = x; self.a[idx] = a; self.t[idx] = t; self.r[idx] = r\n","            self.xn[idx] = xn; self.tn[idx] = tn; self.done[idx] = done\n","            self.ptr = (self.ptr + B) % self.capacity\n","            self.size = int(min(self.capacity, self.size + B))\n","        def sample(self, batch_size):\n","            idx = np.random.randint(0, self.size, size=batch_size)\n","            return (self.x[idx], self.a[idx], self.t[idx], self.r[idx],\n","                    self.xn[idx], self.tn[idx], self.done[idx])\n","        def __len__(self): return self.size\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # Utilities\n","    # ---------------------------------------------------------------------------------------------------\n","    def soft_update(target, source, tau):\n","        return jax.tree_util.tree_map(lambda t, s: (1-tau)*t + tau*s, target, source)\n","\n","    @jit\n","    def predictive_metrics(params_batch, X, y):\n","        \"\"\" Monte-Carlo predictive NLL & accuracy using parameter samples \"\"\"\n","        w = params_batch[:, :-1]            # [S, 30]\n","        b = params_batch[:,  -1:]           # [S, 1]\n","        logits = X @ w.T + b.T              # [N, S]\n","        probs  = jax.nn.sigmoid(logits)     # [N, S]\n","        p_mc   = jnp.mean(probs, axis=1)    # [N]\n","        eps = 1e-7\n","        nll = -jnp.mean(y*jnp.log(p_mc+eps) + (1 - y)*jnp.log(1-p_mc+eps))\n","        acc = jnp.mean((p_mc >= 0.5) == (y >= 0.5))\n","        return nll, acc\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # TD3 Agent for RLFS\n","    # ---------------------------------------------------------------------------------------------------\n","    def make_actor_forward(D_out: int):\n","        module = Actor(out_dim=D_out)\n","        @jax.jit\n","        def _forward(params, x, t):\n","            return module.apply(params, x, t)\n","        return _forward\n","\n","    @jit\n","    def critic_forward(params, x, a, t):\n","        return CriticQ().apply(params, x, a, t)\n","\n","    def twin_critic_grads_factory(actor_forward, target_noise_std, target_noise_clip):\n","        \"\"\"TD3 twin-critic MSE grads with target policy smoothing.\"\"\"\n","        @jax.jit\n","        def _critic_grads(c1_params, c2_params, c1_targ, c2_targ, actor_targ, batch, gamma, key):\n","            x, a, t, r, xn, tn, done = batch\n","\n","            # Target with policy smoothing\n","            noise = target_noise_std * random.normal(key, a.shape)\n","            noise = jnp.clip(noise, -target_noise_clip, target_noise_clip)\n","            a_next = actor_forward(actor_targ, xn, tn) + noise\n","            q1_next = critic_forward(c1_targ, xn, a_next, tn)\n","            q2_next = critic_forward(c2_targ, xn, a_next, tn)\n","            q_next = jnp.minimum(q1_next, q2_next)\n","            y = jax.lax.stop_gradient(r + (1.0 - done) * gamma * q_next)\n","\n","            def loss1(p):\n","                q = critic_forward(p, x, a, t)\n","                return jnp.mean((q - y)**2)\n","            def loss2(p):\n","                q = critic_forward(p, x, a, t)\n","                return jnp.mean((q - y)**2)\n","\n","            g1 = jax.grad(loss1)(c1_params)\n","            g2 = jax.grad(loss2)(c2_params)\n","            return g1, g2\n","        return _critic_grads\n","\n","\n","    def actor_grads_factory(actor_forward):\n","        @jax.jit\n","        def _actor_grads(actor_params, critic_params, batch):\n","            x, _, t, _, _, _, _ = batch\n","            def loss_fn(p):\n","                a_pred = actor_forward(p, x, t)\n","                q = critic_forward(critic_params, x, a_pred, t)\n","                return -jnp.mean(q)\n","            return jax.grad(loss_fn)(actor_params)\n","        return _actor_grads\n","\n","\n","    class TD3Agent:\n","        def __init__(self, D_out=D, sigma=0.2, T=24, lr_actor=3e-4, lr_critic=3e-4,\n","                    tau=0.01, gamma=1.0, seed=0,\n","                    target_noise_std=0.10, target_noise_clip=0.20,\n","                    policy_delay=2, reward_norm=False):\n","            self.sigma, self.T, self.tau, self.gamma = float(sigma), int(T), float(tau), float(gamma)\n","            self.D_out = int(D_out)\n","            self.target_noise_std = float(target_noise_std)\n","            self.target_noise_clip = float(target_noise_clip)\n","            self.policy_delay = int(policy_delay)\n","            self.reward_norm = bool(reward_norm)\n","\n","            key = random.PRNGKey(seed)\n","            dummy_x = jnp.zeros((1, self.D_out)); dummy_a = jnp.zeros((1, self.D_out)); dummy_t = jnp.zeros((1,))\n","            key, ka, kc1, kc2 = random.split(key, 4)\n","\n","            self.actor = Actor(out_dim=self.D_out)\n","            self.critic1 = CriticQ()\n","            self.critic2 = CriticQ()\n","\n","            actor_params = self.actor.init(ka, dummy_x, dummy_t)\n","            c1_params = self.critic1.init(kc1, dummy_x, dummy_a, dummy_t)\n","            c2_params = self.critic2.init(kc2, dummy_x, dummy_a, dummy_t)\n","\n","            clip = optax.clip_by_global_norm(1.0)\n","            self.actor_state = TrainState.create(\n","                apply_fn=self.actor.apply, params=actor_params,\n","                tx=optax.chain(clip, optax.adam(lr_actor))\n","            )\n","            self.critic1_state = TrainState.create(\n","                apply_fn=self.critic1.apply, params=c1_params,\n","                tx=optax.chain(clip, optax.adam(lr_critic))\n","            )\n","            self.critic2_state = TrainState.create(\n","                apply_fn=self.critic2.apply, params=c2_params,\n","                tx=optax.chain(clip, optax.adam(lr_critic))\n","            )\n","\n","            self.actor_targ  = self.actor_state.params\n","            self.critic1_targ= self.critic1_state.params\n","            self.critic2_targ= self.critic2_state.params\n","\n","            self.key = key\n","            self.actor_forward = make_actor_forward(self.D_out)\n","            self._critic_grads = twin_critic_grads_factory(self.actor_forward,\n","                                                          self.target_noise_std,\n","                                                          self.target_noise_clip)\n","            # Standard TD3: use Q1 for policy gradient\n","            self._actor_grads  = actor_grads_factory(self.actor_forward)\n","\n","            # reward normalization stats (optional)\n","            self._r_mean = 0.0\n","            self._r_var = 1.0\n","            self._r_count = 1e-6\n","\n","        @staticmethod\n","        @jit\n","        def _soft_update(target, source, tau):\n","            return jax.tree_util.tree_map(lambda t, s: (1-tau)*t + tau*s, target, source)\n","\n","        def _update_r_stats(self, r_np):\n","            # Welford\n","            r = r_np.reshape(-1).astype(np.float64)\n","            batch_count = r.shape[0]\n","            if batch_count == 0:\n","                return\n","            batch_mean = r.mean()\n","            batch_var  = r.var()\n","            delta = batch_mean - self._r_mean\n","            tot_count = self._r_count + batch_count\n","            new_mean = self._r_mean + delta * batch_count / tot_count\n","            m_a = self._r_var * self._r_count\n","            m_b = batch_var * batch_count\n","            M2 = m_a + m_b + (delta**2) * self._r_count * batch_count / tot_count\n","            new_var = M2 / tot_count\n","            self._r_mean, self._r_var, self._r_count = new_mean, new_var, tot_count\n","\n","        def normalize_r(self, r_np):\n","            if not self.reward_norm:\n","                return r_np\n","            self._update_r_stats(r_np)\n","            std = np.sqrt(self._r_var) + 1e-6\n","            return (r_np - self._r_mean) / std\n","\n","        def update(self, batch, step_idx):\n","            # Optional reward normalization\n","            x, a, t, r, xn, tn, done = batch\n","            r = self.normalize_r(r)\n","            batch_dev = (device_put(jnp.asarray(x)), device_put(jnp.asarray(a)),\n","                        device_put(jnp.asarray(t)), device_put(jnp.asarray(r)),\n","                        device_put(jnp.asarray(xn)), device_put(jnp.asarray(tn)),\n","                        device_put(jnp.asarray(done)))\n","\n","            # Twin critic update\n","            self.key, sub = random.split(self.key)\n","            gC1, gC2 = self._critic_grads(self.critic1_state.params, self.critic2_state.params,\n","                                          self.critic1_targ, self.critic2_targ,\n","                                          self.actor_targ, batch_dev, self.gamma, sub)\n","            self.critic1_state = self.critic1_state.apply_gradients(grads=gC1)\n","            self.critic2_state = self.critic2_state.apply_gradients(grads=gC2)\n","\n","            # Delayed actor update + soft target updates\n","            if (step_idx % self.policy_delay) == 0:\n","                gA = self._actor_grads(self.actor_state.params, self.critic1_state.params, batch_dev)\n","                self.actor_state = self.actor_state.apply_gradients(grads=gA)\n","                self.actor_targ   = self._soft_update(self.actor_targ,  self.actor_state.params,  self.tau)\n","                self.critic1_targ = self._soft_update(self.critic1_targ,self.critic1_state.params,self.tau)\n","                self.critic2_targ = self._soft_update(self.critic2_targ,self.critic2_state.params,self.tau)\n","\n","    # ---------------------------------------------------------------------------------------------------\n","    # Training (TD3-RLFS) on Breast Cancer\n","    # ---------------------------------------------------------------------------------------------------\n","    if __name__ == '__main__':\n","        # Hyperparams (kept same spirit as your previous TD3 run)\n","        SIGMA = 0.1\n","        T_H = 10\n","        B_COLLECT = 2048\n","        REPLAY_CAP = 2048\n","        START_STEPS = 2048\n","        TRAIN_ITERS = 5000\n","        B_UPDATE = 2048\n","        UPDATES_PER_ITER = 1\n","        TAU = 0.01\n","        LR_ACTOR = 1e-5\n","        LR_CRITIC= 5e-5\n","        ALPHA = 3.0    # BLR prior precision\n","\n","        # TD3 knobs\n","        TARGET_NOISE_STD = 0\n","        TARGET_NOISE_CLIP= 0\n","        POLICY_DELAY = 1\n","        REWARD_NORM = False\n","\n","        # Agent & replay\n","        agent = TD3Agent(D_out=D, sigma=SIGMA, T=T_H, lr_actor=LR_ACTOR, lr_critic=LR_CRITIC,\n","                        tau=TAU, gamma=1.0, seed=0,\n","                        target_noise_std=TARGET_NOISE_STD, target_noise_clip=TARGET_NOISE_CLIP,\n","                        policy_delay=POLICY_DELAY, reward_norm=REWARD_NORM)\n","        rb = ReplayBuffer(REPLAY_CAP, obs_dim=D, act_dim=D)\n","        rollout_trajectory = make_rollout_trajectory(T_H, actor_forward=agent.actor_forward)\n","\n","        # Terminal reward helper based on training data\n","        def batch_log_unnorm_pi(xT_batch):\n","            return jax.vmap(lambda params: blr_log_unnormalized(params, X_train, y_train, ALPHA))(xT_batch)\n","\n","        # Warmup collection\n","        print('Collecting warmup...')\n","        for _ in trange(max(1, START_STEPS // B_COLLECT)):\n","            agent.key, sub = random.split(agent.key)\n","            x0 = 0.5 * random.normal(sub, (B_COLLECT, D)); t0 = jnp.zeros((B_COLLECT,))\n","            (xs, as_, ts, rs, xns, tns), (xT, tT) = rollout_trajectory(agent.key, x0, t0, agent.actor_state.params, SIGMA)\n","            r_term = batch_log_unnorm_pi(xT)\n","            rs = rs.at[-1].set(rs[-1] + r_term)\n","            DONE_mat = np.zeros((T_H, B_COLLECT), dtype=np.float32); DONE_mat[-1,:] = 1.0\n","            Xb  = np.array(xs.reshape(-1, D));  A  = np.array(as_.reshape(-1, D))\n","            TT  = np.array(ts.reshape(-1));     R  = np.array(rs.reshape(-1))\n","            XNb = np.array(xns.reshape(-1, D)); TN = np.array(tns.reshape(-1))\n","            DONE = DONE_mat.reshape(-1)\n","            rb.push_batch(Xb, A, TT, R, XNb, TN, DONE)\n","\n","        # Train TD3-RLFS; periodically evaluate\n","        print('Training TD3-RLFS...')\n","        last_params_RL = None\n","        step_idx = 0\n","        for it in trange(TRAIN_ITERS):\n","            agent.key, sub = random.split(agent.key)\n","            x0 = 0.5 * random.normal(sub, (B_COLLECT, D)); t0 = jnp.zeros((B_COLLECT,))\n","            (xs, as_, ts, rs, xns, tns), (xT, tT) = rollout_trajectory(agent.key, x0, t0, agent.actor_state.params, SIGMA)\n","            r_term = batch_log_unnorm_pi(xT)\n","            rs = rs.at[-1].add(r_term)\n","\n","            DONE_mat = np.zeros((T_H, B_COLLECT), dtype=np.float32); DONE_mat[-1,:] = 1.0\n","            Xb  = np.array(xs.reshape(-1, D));  A  = np.array(as_.reshape(-1, D))\n","            TT  = np.array(ts.reshape(-1));     R  = np.array(rs.reshape(-1))\n","            XNb = np.array(xns.reshape(-1, D)); TN = np.array(tns.reshape(-1))\n","            DONE = DONE_mat.reshape(-1)\n","            rb.push_batch(Xb, A, TT, R, XNb, TN, DONE)\n","\n","            if len(rb) >= START_STEPS:\n","                for _ in range(UPDATES_PER_ITER):\n","                    batch = rb.sample(B_UPDATE)\n","                    agent.update(batch, step_idx)\n","                    step_idx += 1\n","\n","            if (it + 1) % 100 == 0:\n","                # Evaluate by rolling many terminal params and computing predictive metrics\n","                agent.key, sub = random.split(agent.key)\n","                x0_eval = 0.5 * random.normal(sub, (8000, D)); t0_eval = jnp.zeros((8000,))\n","                (_, _, _, _, _, _), (params_T, _) = rollout_trajectory(agent.key, x0_eval, t0_eval,\n","                                                                      agent.actor_state.params, agent.sigma)\n","                nll_tr, acc_tr = predictive_metrics(params_T, X_train, y_train)\n","                nll_te, acc_te = predictive_metrics(params_T, X_test,  y_test)\n","                print(f\"[Iter {it+1:04d}] TD3-RLFS — Train NLL={float(nll_tr):.3f} Acc={float(acc_tr):.3f} | \"\n","                      f\"Test NLL={float(nll_te):.3f} Acc={float(acc_te):.3f}\")\n","                last_params_RL = np.array(params_T)\n","\n","        # If loop ended before eval block, sample once\n","        if last_params_RL is None:\n","            agent.key, sub = random.split(agent.key)\n","            x0_eval = 0.5 * random.normal(sub, (8000, D)); t0_eval = jnp.zeros((8000,))\n","            (_, _, _, _, _, _), (params_T, _) = rollout_trajectory(agent.key, x0_eval, t0_eval,\n","                                                                  agent.actor_state.params, agent.sigma)\n","            last_params_RL = np.array(params_T)\n","\n","        # Final test metrics\n","        nll_te, acc_te = predictive_metrics(jnp.asarray(last_params_RL), X_test, y_test)\n","        print(f\"\\n==================== TD3-RLFS FINAL ====================\")\n","        print(f\"Test NLL={float(nll_te):.3f}, Acc={float(acc_te):.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2MpU_JQDltZ","outputId":"84549b4d-0290-4996-d9d7-c6cffa5decc9","executionInfo":{"status":"ok","timestamp":1761851127828,"user_tz":-480,"elapsed":645390,"user":{"displayName":"Yap Vit Chun","userId":"13946168158309770358"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Devices: [CudaDevice(id=0)]\n","Dataset: train=426, test=143, dim=30\n","Collecting warmup...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Training TD3-RLFS...\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 101/5000 [00:16<44:53,  1.82it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0100] TD3-RLFS — Train NLL=3.708 Acc=0.310 | Test NLL=3.490 Acc=0.301\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 201/5000 [00:29<10:25,  7.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0200] TD3-RLFS — Train NLL=6.666 Acc=0.094 | Test NLL=7.032 Acc=0.112\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▌         | 301/5000 [00:41<09:57,  7.86it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0300] TD3-RLFS — Train NLL=2.413 Acc=0.202 | Test NLL=2.797 Acc=0.210\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 401/5000 [00:53<09:21,  8.19it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0400] TD3-RLFS — Train NLL=1.541 Acc=0.359 | Test NLL=1.515 Acc=0.378\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 501/5000 [01:06<08:56,  8.38it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0500] TD3-RLFS — Train NLL=1.462 Acc=0.765 | Test NLL=1.501 Acc=0.713\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 601/5000 [01:18<08:56,  8.20it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0600] TD3-RLFS — Train NLL=2.795 Acc=0.383 | Test NLL=2.916 Acc=0.329\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 701/5000 [01:31<08:41,  8.25it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0700] TD3-RLFS — Train NLL=0.577 Acc=0.707 | Test NLL=0.726 Acc=0.685\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 801/5000 [01:45<08:37,  8.11it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0800] TD3-RLFS — Train NLL=2.837 Acc=0.246 | Test NLL=2.364 Acc=0.371\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 901/5000 [01:57<08:13,  8.30it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 0900] TD3-RLFS — Train NLL=2.559 Acc=0.221 | Test NLL=2.339 Acc=0.252\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 1001/5000 [02:10<07:59,  8.34it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1000] TD3-RLFS — Train NLL=0.404 Acc=0.840 | Test NLL=0.530 Acc=0.825\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 1101/5000 [02:22<07:45,  8.38it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1100] TD3-RLFS — Train NLL=1.335 Acc=0.648 | Test NLL=1.303 Acc=0.678\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 1201/5000 [02:35<07:42,  8.21it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1200] TD3-RLFS — Train NLL=0.645 Acc=0.796 | Test NLL=0.837 Acc=0.783\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 1301/5000 [02:47<07:18,  8.44it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1300] TD3-RLFS — Train NLL=0.960 Acc=0.789 | Test NLL=1.163 Acc=0.804\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 1401/5000 [03:00<07:09,  8.38it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1400] TD3-RLFS — Train NLL=0.438 Acc=0.894 | Test NLL=0.661 Acc=0.853\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 1501/5000 [03:12<06:56,  8.40it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1500] TD3-RLFS — Train NLL=0.492 Acc=0.840 | Test NLL=0.656 Acc=0.818\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 1601/5000 [03:25<06:46,  8.36it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1600] TD3-RLFS — Train NLL=1.507 Acc=0.700 | Test NLL=1.628 Acc=0.664\n"]},{"output_type":"stream","name":"stderr","text":[" 34%|███▍      | 1701/5000 [03:38<06:40,  8.24it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1700] TD3-RLFS — Train NLL=2.030 Acc=0.505 | Test NLL=2.030 Acc=0.490\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 1801/5000 [03:51<06:28,  8.24it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1800] TD3-RLFS — Train NLL=2.682 Acc=0.218 | Test NLL=2.266 Acc=0.259\n"]},{"output_type":"stream","name":"stderr","text":[" 38%|███▊      | 1901/5000 [04:03<06:06,  8.46it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 1900] TD3-RLFS — Train NLL=3.240 Acc=0.169 | Test NLL=2.902 Acc=0.217\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 2001/5000 [04:15<05:58,  8.36it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2000] TD3-RLFS — Train NLL=5.594 Acc=0.249 | Test NLL=5.759 Acc=0.266\n"]},{"output_type":"stream","name":"stderr","text":[" 42%|████▏     | 2101/5000 [04:28<05:45,  8.40it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2100] TD3-RLFS — Train NLL=9.451 Acc=0.131 | Test NLL=9.095 Acc=0.140\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 2201/5000 [04:41<05:35,  8.35it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2200] TD3-RLFS — Train NLL=10.021 Acc=0.106 | Test NLL=10.030 Acc=0.084\n"]},{"output_type":"stream","name":"stderr","text":[" 46%|████▌     | 2301/5000 [04:53<05:19,  8.45it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2300] TD3-RLFS — Train NLL=4.229 Acc=0.347 | Test NLL=4.109 Acc=0.357\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 2401/5000 [05:06<05:12,  8.31it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2400] TD3-RLFS — Train NLL=1.120 Acc=0.648 | Test NLL=1.190 Acc=0.692\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 2501/5000 [05:18<05:01,  8.30it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2500] TD3-RLFS — Train NLL=0.783 Acc=0.800 | Test NLL=0.933 Acc=0.776\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 2601/5000 [05:31<04:48,  8.32it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2600] TD3-RLFS — Train NLL=1.309 Acc=0.803 | Test NLL=1.738 Acc=0.762\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 2701/5000 [05:43<04:46,  8.01it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2700] TD3-RLFS — Train NLL=1.460 Acc=0.793 | Test NLL=1.726 Acc=0.734\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 2801/5000 [06:02<04:30,  8.12it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2800] TD3-RLFS — Train NLL=1.156 Acc=0.791 | Test NLL=1.418 Acc=0.741\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 2901/5000 [06:18<04:15,  8.23it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 2900] TD3-RLFS — Train NLL=1.087 Acc=0.866 | Test NLL=1.209 Acc=0.804\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 3001/5000 [06:31<03:57,  8.42it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3000] TD3-RLFS — Train NLL=0.766 Acc=0.887 | Test NLL=0.740 Acc=0.853\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 3101/5000 [06:43<03:48,  8.30it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3100] TD3-RLFS — Train NLL=0.789 Acc=0.885 | Test NLL=0.625 Acc=0.881\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 3201/5000 [06:56<03:32,  8.45it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3200] TD3-RLFS — Train NLL=0.879 Acc=0.866 | Test NLL=0.819 Acc=0.860\n"]},{"output_type":"stream","name":"stderr","text":[" 66%|██████▌   | 3301/5000 [07:09<03:31,  8.05it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3300] TD3-RLFS — Train NLL=1.148 Acc=0.845 | Test NLL=1.310 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 3401/5000 [07:21<03:08,  8.49it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3400] TD3-RLFS — Train NLL=1.219 Acc=0.847 | Test NLL=1.314 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 3501/5000 [07:34<03:01,  8.28it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3500] TD3-RLFS — Train NLL=1.229 Acc=0.845 | Test NLL=1.433 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 3601/5000 [07:46<02:48,  8.30it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3600] TD3-RLFS — Train NLL=1.338 Acc=0.845 | Test NLL=1.478 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 74%|███████▍  | 3701/5000 [08:00<02:33,  8.46it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3700] TD3-RLFS — Train NLL=1.218 Acc=0.847 | Test NLL=1.413 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 3801/5000 [08:12<02:27,  8.15it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3800] TD3-RLFS — Train NLL=1.330 Acc=0.847 | Test NLL=1.553 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 3901/5000 [08:25<02:44,  6.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 3900] TD3-RLFS — Train NLL=1.387 Acc=0.847 | Test NLL=1.630 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 4001/5000 [08:37<02:43,  6.10it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4000] TD3-RLFS — Train NLL=1.362 Acc=0.845 | Test NLL=1.516 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 82%|████████▏ | 4101/5000 [08:50<02:30,  5.98it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4100] TD3-RLFS — Train NLL=1.364 Acc=0.847 | Test NLL=1.598 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 4201/5000 [09:03<02:19,  5.75it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4200] TD3-RLFS — Train NLL=1.417 Acc=0.845 | Test NLL=1.652 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 86%|████████▌ | 4301/5000 [09:15<02:08,  5.42it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4300] TD3-RLFS — Train NLL=1.388 Acc=0.845 | Test NLL=1.637 Acc=0.790\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 4401/5000 [09:28<01:35,  6.26it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4400] TD3-RLFS — Train NLL=1.448 Acc=0.845 | Test NLL=1.667 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 4501/5000 [09:40<01:09,  7.20it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4500] TD3-RLFS — Train NLL=1.376 Acc=0.845 | Test NLL=1.598 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 4601/5000 [09:53<00:49,  8.06it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4600] TD3-RLFS — Train NLL=1.372 Acc=0.845 | Test NLL=1.538 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 94%|█████████▍| 4701/5000 [10:06<00:36,  8.13it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4700] TD3-RLFS — Train NLL=1.416 Acc=0.845 | Test NLL=1.651 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 4801/5000 [10:19<00:24,  8.17it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4800] TD3-RLFS — Train NLL=1.358 Acc=0.843 | Test NLL=1.627 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 4901/5000 [10:31<00:12,  8.14it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 4900] TD3-RLFS — Train NLL=1.498 Acc=0.843 | Test NLL=1.766 Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5000/5000 [10:43<00:00,  7.76it/s]"]},{"output_type":"stream","name":"stdout","text":["[Iter 5000] TD3-RLFS — Train NLL=1.483 Acc=0.843 | Test NLL=1.739 Acc=0.797\n","\n","==================== TD3-RLFS FINAL ====================\n","Test NLL=1.739, Acc=0.797\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}